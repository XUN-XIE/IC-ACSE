{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ACSE-7 (Inversion and Optimisation)\n",
    "\n",
    "# Homework Lecture 8: Data Assimilation (answers) \n",
    "\n",
    "\n",
    "### Probabilities\n",
    "\n",
    "\n",
    " 1. Using the identity $\\left(\\sum_{i=1}^n a_i\\right)^2=\\sum_{i=1}^n a_i^2+\\sum_{i=1}^n\\sum_{j\\neq i} a_i  a_j$ we have\n",
    "$$ E\\left(\\left[\\sum_i X_i^2\\right]-\\frac{1}{n}\\left[\\sum_i X_i\\right]^2\\right) = E\\left(\\left[\\sum_i X_i^2\\right]-\\frac{1}{n}\\left[\\sum_i X_i^2\\right]-\\frac{1}{n}\\sum_{i=1}^n\\sum_{j\\neq i} X_i X_j\\right) $$\n",
    "Since $E$ commutes with addition we may then write\n",
    "$$ E\\left(\\left[\\sum_i X_i^2\\right]-\\frac{1}{n}\\left[\\sum_i X_i\\right]^2\\right) = \\left(1-\\frac{1}{n}\\right)\\left[\\sum_i E\\left(X_i^2\\right)\\right]-\\frac{1}{n}\\sum_{i=1}^n\\sum_{j\\neq i} E\\left(X_i X_j\\right).$$\n",
    "Now using the definitions of $\\sigma^2$, and $\\mu$ and the fact that $X_i$ and $X_j$ are independent for $i\\neq j$,\n",
    "$$\\begin{eqnarray*} E\\left(\\left[\\sum_i X_i^2\\right]-\\frac{1}{n}\\left[\\sum_i X_i\\right]^2\\right) =& \\left(1-\\frac{1}{n}\\right)\\sum_i \\left[ \\sigma^2 +\\mu^2 \\right]+\\frac{1}{n}\\sum_{i=1}^n\\sum_{j\\neq i} E\\left(X_i\\right) E\\left(X_j\\right),\\\\\n",
    "=& \\left(1-\\frac{1}{n}\\right)\\sum_i \\left[ \\sigma^2 +\\mu^2 \\right]-\\frac{1}{n}\\sum_{i=1}^n\\sum_{j\\neq i} \\mu^2.\\end{eqnarray*}$$\n",
    "Finally, doing the summations and cancelling the $\\mu$s\n",
    "$$ E\\left(\\left[\\sum_i X_i^2\\right]-\\frac{1}{n}\\left[\\sum_i X_i\\right]^2\\right) = (n-1)\\sigma^2 $$\n",
    "\n",
    "2. Multiplying out,\n",
    "$$ E \\left( [X-\\mu_X][Y-\\mu_Y]\\right) = E(XY -\\mu_XY-X\\mu_Y+\\mu_X\\mu_Y).$$\n",
    "Since $E$ commutes with addition and with scalar multiplication\n",
    "$$ E \\left( [X-\\mu_X][Y-\\mu_Y]\\right) = E(XY) -\\mu_XE(Y)-E(X)\\mu_Y+\\mu_X\\mu_Y.$$\n",
    "Now $E(X)=\\mu_X$ and $E(Y)=\\mu_Y$, hence as claimed\n",
    "$$ E \\left( [X-\\mu_X][Y-\\mu_Y]\\right) = E(XY) -\\mu_X\\mu_Y$$\n",
    "3. One possible implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as p\n",
    "\n",
    "def polar_normal_algorithm(mu=0, sigma=1.0, N=1):\n",
    "    counter = 0\n",
    "    tol = 1.0e-16\n",
    "    a = np.empty(N)\n",
    "    b = np.empty(N)\n",
    "    while counter<N:\n",
    "        u, v = np.random.uniform(-1, 1, size=2)\n",
    "        s = u**2+v**2\n",
    "        if s<tol or s>1.0:\n",
    "            continue\n",
    "        c = np.sqrt(-2*np.log(s)/s)\n",
    "        a[counter] = sigma*u*c+mu\n",
    "        b[counter] = sigma*v*c+mu\n",
    "        counter += 1\n",
    "    return a, b\n",
    "\n",
    "a, b = polar_normal_algorithm(1, 2, N=10000)\n",
    "\n",
    "p.hist(a, density=True, alpha=0.4)\n",
    "p.hist(b, density=True, alpha=0.4)\n",
    "x = np.linspace(-10, 10)\n",
    "def p_x(x):\n",
    "    return 1/np.sqrt(8*np.pi)*np.exp(-(x-1)**2/8.0)\n",
    "p.plot(x, p_x(x));\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The BLUE\n",
    "\n",
    "1. For the case of $n$ independent unbiased observations, $T_i$ of a variable, $T$, we can write our linear estimator as\n",
    "$T_{BLUE} = \\sum_{i=1}^n a_i T_i.$\n",
    "Defining our errors as $e_i = T_i -T$, the expected error in $T_{BLUE}$ is\n",
    "$$E(T_{BLUE}-T) = E\\left(T\\left(1-\\sum_{i=1}^n a_i\\right) +\\sum_{i=1}^n a_i e_i\\right)=T\\left(1-\\sum_{i=1}^n a_i\\right),$$\n",
    "so to be unbiased $\\sum_{i=1}^n a_i=1$. Now for unbiased estiamtesthe expectation of the square error is\n",
    "$$E((T_{BLUE}-T)^2) = E((\\sum_{i=1}^n a_i e_i)^2)=\\sum_i a_i^2 \\sigma_i^2,$$\n",
    "where $\\sigma_i^2$ is the variance of the error in the $i$th observation and we have used the independence of the errors. We'll minimise this using a Lagrange multiplier, so\n",
    "$$\\mathcal{J}=\\lambda\\left(1-\\sum_{i=1}^n a_i\\right)+\\sum_i a_i^2 \\sigma_i^2$$\n",
    "and our optimal choice satisfies\n",
    "$$ \\sum_{i=1}^n a_i = 1, $$\n",
    "$$ 2a_i\\sigma_i^2 = \\lambda. $$\n",
    "\n",
    "The second equation implies $a_i=\\frac{\\lambda}{\\sigma_i^2}$ while the first then gives\n",
    "$$\\lambda\\left[\\sum\\frac{1}{\\sigma_i^2}\\right]=1\\Rightarrow \\lambda = \\frac{1}{\\sum_{i=1}^n\\frac{1}{\\sigma_i^2}}$$ so that\n",
    "$$ a_i = \\frac{1}{\\sum_{j=1}^n\\frac{\\sigma_i^2}{\\sigma_j^2}}.$$\n",
    "Finally, our BLUE is\n",
    "$$T_{BLUE} = \\frac{1}{\\sum_{j=1}^n\\frac{1}{\\sigma_j^2}}\\sum_{i=1}^n \\left[\\frac{T_i}{\\sigma_i^2}\\right].$$\n",
    "2. Consider our usual linear combination of our observations,\n",
    "$$ T_\\mbox{guess}=\\sum_i a_i T_i = \\sum_i \\left[a_i \\left(T+e_i\\right)\\right]. $$\n",
    "So that (for independent observation errors) our mean square error is\n",
    "$$E(e^2):=E(T_\\mbox{guess}-T_t)=\\sum_i a^2_i \\sigma_i-2 +(1-\\sum_i a_i)^2  T^2 $$ \n",
    "dropping the constraint that the estimator must be unbiased, and thus the $a_i$s must sum to 1, we obtain optimality constraints,\n",
    "$$ a_i(T_t^2+\\sigma_i^2) = (1-\\sum_j a_j)T_t^2. $$\n",
    "\n",
    "Since all the right hand sides are the same for every $i$, we can just call it $\\lambda$ to get $a_i = \\frac{\\lambda}{T_t^+\\sigma_i^2}$ and thus\n",
    "$$ \\lambda = \\left(1-\\lambda\\sum_i \\frac{1}{T_t^2+\\sigma_i^2}\\right) $$\n",
    "or in other words,\n",
    "$$ a_i = \\frac {1}{T_t^2 + \\sigma_i}.$$\n",
    "\n",
    "Although this choice of $a_i$ does indeed have a smaller error than the BLUE, it also depends explicitly on knowledge of $T_t$, which is precisely the thing we are trying to find. This makes it impossiple to use, unless we happen to know $T_t$. As such, this is not a useful realworld estimtor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal interpolation\n",
    "\n",
    "Questions 1 & 2 are purely practical exercises. The real take home here is that data assimilation methods cease to minimise the mean square error in analysis states (or, for variational approaches, to generate a maximum likelihood estimator) as soon as the statitics inserted into them are invalid. In particular, systematic biases need to be identified, or they can significntly skew your analysis estimates.\n",
    "\n",
    "3. In the real world, \"copy-pasted\" observations from the same instrument will be perfectly correlated. This means that the $\\mathbf{R}$ matrix will have a pair of rows (also columns) which are identical (i.e. not linearly independent). This makes the matrix singular, as you learnt in Lecture 1. Fortunately in this case the \"right hand side\", that is the relevant columns of $\\mathbf{y}-\\mathbf{H}x$ will also be identical, since the $\\mathbf{H}$ operator should be the same.  This means we are in the situation with infinite solutions, rather than the more inconvenient case with zero solutions, and in fact the $\\mathbf{H}^T$ operator will fix most of our problems for us, so we could obtain the result we'd expect using a pseudom inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D-Var\n",
    "\n",
    "Here is some sample code with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3D-Var implementation and solution\n",
    "\n",
    "# We will use some weather-like 2d data and generate the B from climatology.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import cg\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "nx = 26\n",
    "ny = 11\n",
    "\n",
    "Lx = 1e6\n",
    "Ly = 4e5\n",
    "\n",
    "U_0 =  30.0\n",
    "radius = 5e4\n",
    "\n",
    "def wind_field(X, Y, circulations, centres):\n",
    "    \n",
    "    U = np.full((ny, nx), U_0)\n",
    "    V = np.zeros((ny, nx))\n",
    "    \n",
    "    for circ, (x, y) in zip(circulations, centres):\n",
    "        \n",
    "        r2= (X-x)**2 + (Y-y)**2\n",
    "        \n",
    "        u = circ/(2*np.pi)*np.where(r2>radius**2, 1./r2, 1.0/radius**2) \n",
    "        \n",
    "        \n",
    "        U -= (Y-y)*u\n",
    "        V += (X-x)*u\n",
    "        \n",
    "    return U, V\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(0,Lx,nx), np.linspace(0,Ly, ny))\n",
    "\n",
    "\n",
    "def random_vortices(N, kx=5, ky=5):\n",
    "    return (200*np.random.lognormal(0, 0.1, size=N)*radius,\n",
    "            np.random.uniform([-kx*radius, -kx*radius], [Lx+ky*radius, Ly+ky*radius], (N, 2)))\n",
    "\n",
    "U_t, V_t = wind_field(X, Y, *random_vortices(4, -1, -1))\n",
    "\n",
    "# observation locations\n",
    "n_full = 25\n",
    "n_speed = 25\n",
    "y_loc = np.random.randint(0, nx*ny, n_full+n_speed)\n",
    "# observation values\n",
    "y = np.empty(2*n_full+n_speed)\n",
    "y[:n_full] = U_t.ravel()[y_loc[:n_full]] + np.random.normal(0, 2.0, n_full)\n",
    "y[n_full:2*n_full] = V_t.ravel()[y_loc[:n_full]] + np.random.normal(0, 2.0, n_full)\n",
    "y[2*n_full:] = (np.sqrt(U_t.ravel()[y_loc[n_full:]]**2\n",
    "                      + V_t.ravel()[y_loc[n_full:]]**2)\n",
    "                      + np.random.normal(0, 2, n_speed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    hx = np.empty(2*n_full+n_speed)\n",
    "    u = x[y_loc]\n",
    "    v = x[ny*nx+y_loc]\n",
    "    hx[:n_full] = u[:n_full]\n",
    "    hx[n_full:2*n_full] = v[:n_full]\n",
    "    hx[2*n_full:] = np.sqrt(u[n_full:]**2+v[n_full:]**2)\n",
    "    \n",
    "    return hx\n",
    "\n",
    "def H(x):\n",
    "    H_x = np.zeros((2*n_full+n_speed, 2*nx*ny))\n",
    "    u = x[y_loc]\n",
    "    v = x[ny*nx+y_loc]\n",
    "    \n",
    "    # To avoid problems with division by zero, we add a small\n",
    "    # factor to the denominator.\n",
    "    vel = np.sqrt(u[n_full:]**2+v[n_full:]**2)+1.0e-16\n",
    "    for _ in range(n_full):\n",
    "      H_x[_, y_loc[_]] = 1.0\n",
    "      H_x[n_full+_, ny*nx+y_loc[_]] = 1.0\n",
    "    for _ in range(n_speed):\n",
    "      H_x [2*n_full+_, y_loc[n_full+_]] = u[n_full+_]/vel[_]\n",
    "      H_x [2*n_full+_, ny*nx+y_loc[n_full+_]] = v[n_full+_]/vel[_]\n",
    "    \n",
    "    return H_x\n",
    "    \n",
    "\n",
    "R = 2.0**2*np.eye(2*n_full+n_speed)\n",
    "\n",
    "U = np.empty((5000,ny,nx))\n",
    "V = np.empty((5000,ny,nx))\n",
    "\n",
    "for _ in range(U.shape[0]):\n",
    "    U[_, : :], V[_, :, :] = wind_field(X, Y, *random_vortices(4))\n",
    "\n",
    "mu_u = np.mean(U, 0)\n",
    "mu_v = np.mean(V, 0)\n",
    "\n",
    "d = np.empty((U.shape[0], 2*ny*nx))\n",
    "for _ in range(d.shape[0]):\n",
    "    d[_, :ny*nx] = (U[_, :]-mu_u).ravel() \n",
    "    d[_, ny*nx:] = (V[_, :]-mu_v).ravel()\n",
    "    \n",
    "B = np.empty((2*nx*ny, 2*nx*ny))\n",
    "for i in range(2*nx*ny):\n",
    "    for j in range(2*nx*ny):\n",
    "        B[i, j] = np.sum(d[:, i]*d[:, j])/(U.shape[0]-1)\n",
    "        \n",
    "x_b = np.empty(2*ny*nx)\n",
    "\n",
    "x_b[:ny*nx] = mu_u.ravel()\n",
    "x_b[ny*nx:] = mu_v.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       50, 51, 51, 52, 52, 53, 53, 54, 54, 55, 55, 56, 56, 57, 57, 58, 58,\n",
      "       59, 59, 60, 60, 61, 61, 62, 62, 63, 63, 64, 64, 65, 65, 66, 66, 67,\n",
      "       67, 68, 68, 69, 69, 70, 70, 71, 71, 72, 72, 73, 73, 74, 74]), array([248, 244,  81, 125, 274,  76,  33, 262,  20, 158, 251, 134,  23,\n",
      "        44, 254,  41, 125, 172,  49,  64, 150, 259,  45, 153, 154, 534,\n",
      "       530, 367, 411, 560, 362, 319, 548, 306, 444, 537, 420, 309, 330,\n",
      "       540, 327, 411, 458, 335, 350, 436, 545, 331, 439, 440, 137, 423,\n",
      "       236, 522, 254, 540,  28, 314, 156, 442, 254, 540, 203, 489, 150,\n",
      "       436,  91, 377, 217, 503,  23, 309, 155, 441,  72, 358, 279, 565,\n",
      "        46, 332, 208, 494, 118, 404, 234, 520,  18, 304, 105, 391, 141,\n",
      "       427,  16, 302,  82, 368,  31, 317,  63, 349]))\n"
     ]
    }
   ],
   "source": [
    "print(H(x_b).nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Binv(z0):\n",
    "    \"Iterative solver for inverse of B matrix.\"\n",
    "    z = z0/B.diagonal()\n",
    "    z, info = cg(B, z0, z, maxiter=10, atol=1e-8)\n",
    "    return z\n",
    "\n",
    "def Rinv(z0):\n",
    "    \"Fast diagonal solver for R matrix.\"\n",
    "    return z0/R.diagonal()\n",
    "\n",
    "\n",
    "def J(x):\n",
    "    dx_b = x-x_b\n",
    "    dx_o = y - h(x)\n",
    "    return np.dot(dx_b, Binv(dx_b))+np.dot(dx_o, Rinv(dx_o))\n",
    "\n",
    "def jac(x):\n",
    "    dx_b = x-x_b\n",
    "    dx_o = y - h(x)\n",
    "    \n",
    "    return 2.0*Binv(dx_b)-2.0*H(x).T.dot(Rinv(dx_o))\n",
    "\n",
    "def callback(x):\n",
    "    print(J(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 2min 16s, total: 3min 42s\n",
      "Wall time: 34.6 s\n",
      "J for x_a: 108.51737703511122\n",
      "CPU times: user 215 ms, sys: 505 ms, total: 720 ms\n",
      "Wall time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "# Method using finite difference gradients\n",
    "%time res = minimize(J, x_b, method='CG', tol=1e-3, options={'maxiter':10})\n",
    "print('J for x_a:', J(res.x))\n",
    "# Method provided a Jacobian\n",
    "%time res = minimize(J, x_b,  jac=jac, method='CG', tol=1e-3, options={'maxiter':100})\n",
    "x_a = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J for x_b: 2054.4771987630143\n",
      "J for x_a: 98.6461129804279\n"
     ]
    }
   ],
   "source": [
    "print('J for x_b:', J(x_b))\n",
    "print('J for x_a:', J(x_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The equations are\n",
    "$$ \\mathbf{R}{a} + H\\mathbf{b} = \\mathbf{c},$$\n",
    "$$ \\mathbf{H}^T\\mathbf{a}- \\mathbf{B}^{-1}a\\mathbf{b} = \\mathbf{0}.$$\n",
    "Eliminating $\\mathbf{b}$ gives $\\mathbf{a}= (\\mathbf{R}+\\mathbf{H}\\mathbf{B}\\mathbf{H}^T)^{-1}\\mathbf{c}$. Substituting into the second equation gives\n",
    "$$\\mathbf{b} = \\mathbf{B}\\mathbf{H}^T(\\mathbf{R}+\\mathbf{H}\\mathbf{B}\\mathbf{H}^T)^{-1}\\mathbf{c}.$$\n",
    "Eliminating $\\mathbf{a}$ directly gives \n",
    "$$\\mathbf{b} = (\\mathbf{B}^{-1}+\\mathbf{H}^T\\mathbf{R}^{-1}\\mathbf{H})^{-1}\\mathbf{H}^T\\mathbf{R}^{-1}\\mathbf{c}.$$\n",
    "These are the Optimal interpolation and 3D-Var forms of the increment to the background state. Since the problem is well posed, the two methods must generate the same solution, proving that linear 3D-Var and Optimal Interpolation find the same solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filter\n",
    "\n",
    "1. The basic Kalman update equations for hindcasting become \n",
    "  $$\\mathbf{x}_a^{(k)}= \\mathbf{x}_b^{(k)}+\\mathbf{B}^{(k)}\\mathbf{H}^T(\\mathbf{R}^{(k)}+\\mathbf{H}\\mathbf{B}^{(k)}\\mathbf{H}^T)^{-1}(\\mathbf{y}-\\mathbf{H}\\mathbf{x}_b^{(k)}) $$\n",
    "  $$\\mathbf{P}^{(k)} = (\\mathbf{I}-\\mathbf{B}^{(k)}\\mathbf{H}^T(\\mathbf{R}^{(k)}+\\mathbf{H}\\mathbf{B}^{(k)}\\mathbf{H}^T)^{-1}\\mathbf{H})\\mathbf{B}^{(k)}.$$\n",
    "  $$\\mathbf{x}_b^{(k-1)} = \\mathbf{M}^{-1} \\mathbf{x}_a^{(k)}.$$\n",
    "  $$\\mathbf{B}^{(k-1)} = \\mathbf{M}^{-1} \\mathbf{P}^{(k)} \\left[\\mathbf{M}^{-1}\\right]^T+\\mathbf{Q}.$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4D-Var\n",
    "\n",
    " 1. Rewriting $\\mathbf{x}_0$ as $\\mathbf{M}^{-1}\\mathbf{x}_1$ gives a cost function wholy in $\\mathbf{x}_1$,  \n",
    "$$\\mathcal{J}(\\mathbf{x}_1) = \\frac{1}{2}(\\mathbf{M}^{-1}\\mathbf{x}_1-\\mathbf{x}_b)^T\\mathbf{B}^{-1}(\\mathbf{M}^{-1}\\mathbf{x}_1-\\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{y}_1-\\mathbf{H}\\mathbf{x}_1)^T\\mathbf{R}^{-1}(\\mathbf{y}_1-\\mathbf{H}\\mathbf{x}_1).$$\n",
    "Differentiating this function with respect to $\\mathbf{x}_1$ gives\n",
    "$$\\nabla_{\\mathbf{x}_1}\\mathcal{J}=[\\mathbf{M}^{-1}]^T\\mathbf{B}^{-1}(\\mathbf{M}^{-1}\\mathbf{x}_1-\\mathbf{x}_b)+\\mathbf{H}^T\\mathbf{R}^{-1}(\\mathbf{y}_1-\\mathbf{H}\\mathbf{x}_1).$$\n",
    "The optimal solution is therefore\n",
    "$$ \\mathbf{x}_a^(1) = \\mathbf{M} \\mathbf{x}_b + \\mathbf{M}\\mathbf{B}\\mathbf{M}^T\\mathbf{H}^T(\\mathbf{R}+\\mathbf{H}\\mathbf{M}\\mathbf{B}\\mathbf{M}^T\\mathbf{H}^T)^{-1}(\\mathbf{y}-\\mathbf{H}\\mathbf{M}\\mathbf{x}_b),$$\n",
    "implying that the optimal starting value is\n",
    "$$\\mathbf{x}_a^{(0)} = \\mathbf{M}^{-1}\\mathbf{x}_a^{(1)}=\\mathbf{x}_b+\\mathbf{B}\\mathbf{M}^T\\mathbf{H}^T(\\mathbf{R}^{(k)}+\\mathbf{H}\\mathbf{M}\\mathbf{B}\\mathbf{M}^T\\mathbf{H}^T)^{-1}(\\mathbf{y}-\\mathbf{H}\\mathbf{M}\\mathbf{x}_b)$$\n",
    "This implies that the 4D-Var equation can be interpretted in a number of ways:\n",
    "  - The first equation is a 3D-Var equation for an error covariance matrix $\\mathbf{M}\\mathbf{B}\\mathbf{M}^T$ (remember that for matrices $[ABC^T]^{-1} = [C^T]^{-1}B^{-1}A^{-1}$)\n",
    "and a background state $\\mathbf{M}\\mathbf{x}_b$. In this viewpoint, the forward model is being used pull forward the background state forward and update the error covariance in line with the Kalman gain equations, while the inverse model is used to pull final the analysis state back to the start.\n",
    "  - Alternatively, the system can be seen as pulling the observation operator forward in time (note the repeated  $\\mathbf{H}\\mathbf{M}$), with the background error covariance unchanged.\n",
    " \n",
    " This duality is the basis of the [Kalman Smoother](https://en.wikipedia.org/wiki/Kalman_filter#Fixed-interval_smoothers), which takes the Kalman filter approach and extends it over a fixed number number of data assimilation steps to provide the increment to the *initial* state to minimize the mean square error in the *final* analysis error covariance, rather than working state to state. Linear 4D-Var is thus equivalent to the Kalman smoother in the same way that linear 3D-Var is equivalent to Optimal interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
