{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACSE-7 (Inversion and Optimisation)  <a class=\"tocSkip\"></a>\n",
    "\n",
    "# Homework Lecture 6: Unconstrained Optimisation (solutions) <a class=\"tocSkip\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1pt\">Some $\\LaTeX$ definitions hidden in this cell (double-click to reveal)</font>\n",
    "$\n",
    "\\renewcommand\\vec[1]{\\mathbf{#1}}\n",
    "\\newcommand\\mat[1]{\\underline{\\mathbf{#1}}}\n",
    "\\newcommand\\R{\\mathbb{R}}\n",
    "\\newcommand\\todo[1]{\\textcolor{red}#1}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "import matplotlib.pyplot as plt\n",
    "# font sizes for plots\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Dejavu Sans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Box Shape\n",
    "A chocolate manufacturer wants to design a 6-sided box: six sides and a regular hexagon as the top and bottom:\n",
    "\n",
    "<img width=200 src=\"https://images-na.ssl-images-amazon.com/images/I/41LOrfo46sL._AC_SL1000_.jpg\">\n",
    "\n",
    "For a given volume, what is the optimal ratio between the height $h$, and the radius $r$ of the hexagon in order to minimze the amount of packaging, i.e. to minimize the area of the box. **Hint:** you were given the formula of the area of a regular hexagon in the tutorial for lecture 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Problem\n",
    "\n",
    "Consider the following constrained problem:\n",
    "\n",
    "$$\n",
    "  \\text{minimize}f(x, y) = x^4 + y^4 + 2x^2 \\\\\n",
    "  \\text{subject to }x^2 + y ^2 = 2\n",
    "$$\n",
    "\n",
    "* sketch the feasible region\n",
    "* is this a convex constrained optimisation problem?\n",
    "* find the stationary points of the constrained problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation of the Trust Region method\n",
    "Using the techniques from today's lecture, we can derive the formula that modifies the Hessian in the Trust Region method (lecture 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Given a general quadratic function of the usual form\n",
    "\n",
    "$$\n",
    "  f(\\vec x) = \\frac 12 \\vec x^T \\mat A\\vec x - \\vec b^T\\vec x +c\n",
    "$$\n",
    "\n",
    "find an equation for the minimum $f(\\vec x)$ for points $\\vec x$ *on* the circle (in 2D) or sphere (in 3D) or \"hyper-sphere\" in n dimensions using the following equality constraint:\n",
    "\n",
    "$$\n",
    "  g(\\vec x) = \\frac 12\\vec x^T \\vec x - \\frac 12 r^2 = 0\n",
    "$$\n",
    "\n",
    "You don't need to solve for the Lagrange multiplier (for this see the \"final note\" below), but try arrive an equation in the following form:\n",
    "\n",
    "$$\n",
    "  \\left[\\mat A + \\lambda\\mat I\\right]\\vec x = \n",
    "\\vec b\n",
    "$$\n",
    "\n",
    "where $\\mat I$ is the identity matrix. You may assume that $\\mat A$ is SPD to ensure that a minimum exists, and is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now proof the following expression for the optimisation problem of finding the minimum *inside* (or on the boundary of) a circle/sphere/hyper-sphere of radius $r$:\n",
    "\n",
    "$$\n",
    "  \\left[\\mat A + \\mu\\mat I\\right]\\vec x = \n",
    "\\vec b\n",
    "$$\n",
    "\n",
    "What conditions do we have for $\\mu$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write down the first three terms in the Taylor series approximation of a function $f:\\R^n \\to\\R$ in a point $\\vec x^{(i)}+\\vec p$ near another point $\\vec x^{(i)}$:\n",
    "\n",
    "$$\n",
    "  f(\\vec x^{(i)}+\\vec p) = ...\n",
    "$$\n",
    "\n",
    "Find the minimum of this approximation formed by the three terms (derive an expression to solve for $\\vec p$), and relate your result to the (unconstrained) Newton method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In the trust region method, we first try a standard Newton step, which you've derived again in the previous question. If the resulting $\\vec p$ does not lead to a sufficient decrease in the new function value $f(\\vec x^{(i)}+\\vec p)$, we restrict the region in which we *trust* the quadratic approximation. We do this by choosing a radius $r$ that is smaller than the step size of our first try, and find the minimum of the quadratic approximation with the restriction that $\\|\\vec p\\|\\leq r$. In other words we choose a sphere of radius $r$ around $\\vec x^{(i)}$ in which we trust the approximation. Combine your answers to question 2. and 3. to derive an equation for this trust region approach. Again you may assume that the Hessian in $\\vec x^{(i)}$ is SPD. Why can we assume that the minimum is *not* going to be found in the interior of the radius-$r$ sphere?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final note <a class=\"tocSkip\"></a>\n",
    "In our previous answer, like in the answer to 2. we still have a $\\mu$ that we don't know. We can derive an equation for $\\mu$ by rewriting the result of answer 2 in the form:\n",
    "\n",
    "$$\n",
    "  \\vec x = \\left[\\mat A + \\mu\\mat I\\right]^{-1}\\vec b\n",
    "$$\n",
    "\n",
    "If we assume the minimum is found on the boundary of the sphere, we have\n",
    "\n",
    "$$\n",
    "  \\vec x^T\\vec x = r^2\n",
    "$$\n",
    "\n",
    "which we can work out to\n",
    "\n",
    "\\begin{align*}\n",
    "  \\vec b^T \\left[\\mat A + \\mu\\mat I\\right]^{-1,T}\\left[\\mat A + \\mu\\mat I\\right]^{-1}\\vec b &= r^2 \\\\\n",
    "  \\vec b^T \\left(\\left[\\mat A + \\mu\\mat I\\right]\\left[\\mat A + \\mu\\mat I\\right]^T\\right)^{-1}\\vec b &= r^2\n",
    "\\end{align*}\n",
    "\n",
    "This is an equation that we can solve for $\\mu$. As you can see, it is not a very nice equation to solve. There are therefore various approaches to deal with this situation:\n",
    "\n",
    "* in an *exact trust region approach* we do solve the equation above. We choose an initial radius $r$, which we may for instance halve every time we find a point that does not satisfy a sufficient decrease condition (e.g. Armijo). We then solve the above equation for $\\mu$. It is a one-dimensional nonlinear equation, so in principle we can just solve with for instance Newton. Obviously this might add considerable expense to the algorithm\n",
    "* in other approaches, instead of choosing a $r$ beforehand, we simply treat $\\mu$ as a free parameter. We start using $\\mu=0$ which gives plain Newton, then if the sufficient decrease hasn't been achieved we simply increase $\\mu$. For very big $\\mu$ we get\n",
    "\n",
    "$$\n",
    "  \\mat A+\\mu\\mat I\\approx \\mu\\mat I\n",
    "$$\n",
    "\n",
    "and therefore we have a update step of\n",
    "\n",
    "$$\n",
    "  \\mu\\mat I\\vec p = -f'(\\vec x^{(I)}) \\implies \\vec p =-\\frac 1\\mu f'(\\vec x^{(I)})\n",
    "$$\n",
    "\n",
    "which is just Steepest Descent with a \"line search parameter\" $\\lambda=1/\\mu$. Thus we can choose any $\\nu$ between zero and infinity, where for big $\\mu$ we transform the method into Steepest Descent. So we can use this method in a similar way as backtracking line search, but instead of using a $\\lambda$ that starts at 1 and is decreased until we satisfy a sufficient decrease condition in $f$, we now have a parameter $\\nu$ that we start at zero and keep increasing until the condition is satisfied. For more information see e.g. [Kelley's \"Iterative Methods for Optimization\"](https://www.amazon.com/Iterative-Methods-Optimization-Frontiers-Mathematics/dp/0898714338)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
