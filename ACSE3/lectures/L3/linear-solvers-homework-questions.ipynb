{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACSE-3 (Numerical Methods) <a class=\"tocSkip\">\n",
    "\n",
    "## Lecture 3: Linear Solvers (Numerical Linear Algebra) <a class=\"tocSkip\">\n",
    "    \n",
    "### Homework Exercises  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Homework\" data-toc-modified-id=\"Homework-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Homework</a></span><ul class=\"toc-item\"><li><span><a href=\"#Homework---Gaussian-elimination---3x3-example\" data-toc-modified-id=\"Homework---Gaussian-elimination---3x3-example-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Homework - Gaussian elimination - 3x3 example</a></span></li><li><span><a href=\"#Homework---LU-solve\" data-toc-modified-id=\"Homework---LU-solve-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Homework - LU solve</a></span></li><li><span><a href=\"#Homework---Gauss-Jordan-elimination-(matrix-inversion)\" data-toc-modified-id=\"Homework---Gauss-Jordan-elimination-(matrix-inversion)-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Homework - Gauss-Jordan elimination (matrix inversion)</a></span></li><li><span><a href=\"#Homework---the-Thomas-algorithm-for-solving-tridiagonal-systems--[$\\star$]\" data-toc-modified-id=\"Homework---the-Thomas-algorithm-for-solving-tridiagonal-systems--[$\\star$]-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Homework - the Thomas algorithm for solving tridiagonal systems  [$\\star$]</a></span></li><li><span><a href=\"#Homework---round-off-error-example-(Gauss-Jordan-inversion)\" data-toc-modified-id=\"Homework---round-off-error-example-(Gauss-Jordan-inversion)-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Homework - round off error example (Gauss-Jordan inversion)</a></span></li><li><span><a href=\"#Homework---Iterative-method-3---the-Gauss-Seidel-method\" data-toc-modified-id=\"Homework---Iterative-method-3---the-Gauss-Seidel-method-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Homework - Iterative method 3 - the Gauss-Seidel method</a></span></li><li><span><a href=\"#Homework---Iterative-method-4---SOR-(a)\" data-toc-modified-id=\"Homework---Iterative-method-4---SOR-(a)-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Homework - Iterative method 4 - SOR (a)</a></span></li><li><span><a href=\"#Homework---Iterative-method-4---SOR-(b)--[$\\star$]\" data-toc-modified-id=\"Homework---Iterative-method-4---SOR-(b)--[$\\star$]-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Homework - Iterative method 4 - SOR (b)  [$\\star$]</a></span></li><li><span><a href=\"#Homework---Algorithm-comparison--[$\\star$]\" data-toc-modified-id=\"Homework---Algorithm-comparison--[$\\star$]-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Homework - Algorithm comparison  [$\\star$]</a></span></li><li><span><a href=\"#Homework---Cramer's-rule-[$\\star\\star$]\" data-toc-modified-id=\"Homework---Cramer's-rule-[$\\star\\star$]-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Homework - Cramer's rule [$\\star\\star$]</a></span></li><li><span><a href=\"#Homework---Interpolation-(Vandermonde-&amp;-Newton)-matrices--[$\\star$]\" data-toc-modified-id=\"Homework---Interpolation-(Vandermonde-&amp;-Newton)-matrices--[$\\star$]-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Homework - Interpolation (Vandermonde &amp; Newton) matrices  [$\\star$]</a></span></li><li><span><a href=\"#Homework---Hilbert-matrix--[$\\star$]\" data-toc-modified-id=\"Homework---Hilbert-matrix--[$\\star$]-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>Homework - Hilbert matrix  [$\\star$]</a></span></li><li><span><a href=\"#Homework---Comparison-of-iterative-methods--[$\\star$]\" data-toc-modified-id=\"Homework---Comparison-of-iterative-methods--[$\\star$]-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;</span>Homework - Comparison of iterative methods  [$\\star$]</a></span></li><li><span><a href=\"#Homework---Cubic-splines-(from-Lecture-1)--[$\\star\\star$]\" data-toc-modified-id=\"Homework---Cubic-splines-(from-Lecture-1)--[$\\star\\star$]-1.14\"><span class=\"toc-item-num\">1.14&nbsp;&nbsp;</span>Homework - Cubic splines (from Lecture 1)  [$\\star\\star$]</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%precision 16\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sci\n",
    "import scipy.linalg as sl\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spl\n",
    "\n",
    "# prints in a way that we can cut and paste directly into code\n",
    "from pprint import pprint\n",
    "\n",
    "# font sizes for plots\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Dejavu Sans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Gaussian elimination - 3x3 example \n",
    "\n",
    "Consider now the system of three linear equations for three unknowns:\n",
    "\n",
    "\\begin{align*}\n",
    "  2x + 3y - 4z &= 5, \\\\[5pt]\n",
    "  6x + 8y + 2z &= 3, \\\\[5pt]\n",
    "  4x + 8y - 6z &= 19.\n",
    "\\end{align*}\n",
    "\n",
    "To ensure you understand what our algorithm will need to do, write this in matrix form, form the corresponding augmented system and perform row operations until you get to upper-triangular form, find the solution using back substitution (**do this all with pen and paper**).\n",
    "\n",
    "Check your answer against a solution found using SciPy.\n",
    "\n",
    "You should find $x=-6$, $y=5$, $z=-1/2$.\n",
    "\n",
    "Check also that the upper triangular matrix you obtain in this process is consistent with that obtained by our LU decomposition code (without partial pivoting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - LU solve\n",
    "\n",
    "Write and test a function to replicate \n",
    "[https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html#scipy.linalg.lu_solve](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html#scipy.linalg.lu_solve).\n",
    "\n",
    "You could try to formulate solutions which are based on either `LU_decomposition` or `LU_decomposition_pp`, i.e. with and without the use of partial pivoting.\n",
    "\n",
    "Test your solver against `scipy.linalg.lu_solve` using the problem from lecture:\n",
    "\n",
    "```Python \n",
    "A = np.array([[1., 0., 3., 7.], \n",
    "              [2., 1., 0., 4.],\n",
    "              [5., 4., 1., -2.], \n",
    "              [4., 1., 6., 2.]])\n",
    "b = np.array([1., 2., -3., 2.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Gauss-Jordan elimination (matrix inversion)\n",
    "\n",
    "The (Gaussian) elimination procedure we saw in the lecture is based on the fact that if $\\boldsymbol{b} = A\\boldsymbol{x}$, then row operations (multiplying by a nonzero scalar, and subtracting) on the matrix  $A$ have the same effect on the resulting matrix vector product $A\\boldsymbol{x}$ as when we apply the same operations directly on the vector $\\boldsymbol{b}$. \n",
    "\n",
    "Since we can write $\\boldsymbol{b}=I\\boldsymbol{b}$ where $I$ is the identity matrix, we can change the procedure by starting with the matrix $I$ on the right-hand side of the augmented matrix, and instead of applying the operations on $\\boldsymbol{b}$ directly we apply it to the identity matrix which step-by-step gets transformed into a different matrix $M$. \n",
    "\n",
    "I.e. consider the system \n",
    "\n",
    "$$A\\boldsymbol{x} = \\boldsymbol{b},$$\n",
    "\n",
    "if we apply the same row operations to $A$ as to $\\boldsymbol{b}$, up until $A$ has been transformed into the identity, then we have\n",
    "\n",
    "$$I\\boldsymbol{x} = \\hat{\\boldsymbol{b}},$$\n",
    "\n",
    "where the hat notation indicates that we have performed the same row operations on the matrix $\\boldsymbol{b}$. From which we simply have that $\\boldsymbol{x} = \\hat{\\boldsymbol{b}}$.\n",
    "\n",
    "Instead, suppose we start things off by writing \n",
    "\n",
    "$$A\\boldsymbol{x} = I\\boldsymbol{b},$$\n",
    "\n",
    "and again perform the row operations, but on the RHS now apply these to the matrix $I$ rather than the vector $\\boldsymbol{b}$.  \n",
    "\n",
    "If we continue the procedure until the matrix-part on the left of the augmented matrix has changed to the identity matrix (as in Gauss-Jordan elimination), we end up with a result $M\\boldsymbol{b}$ on the right-hand side for which\n",
    "\n",
    "$$I\\boldsymbol{x} = M{\\boldsymbol{b}},$$\n",
    "\n",
    "which we know should be equal to the solution of $A\\boldsymbol{x}=\\boldsymbol{b}$, in other words $M\\boldsymbol{b}=\\boldsymbol{x}=A^{-1}\\boldsymbol{b}$. \n",
    "\n",
    "But note that we have not used any information about $\\boldsymbol{b}$ within this process, i.e. $M\\boldsymbol{b}=A^{-1}\\boldsymbol{b}$ holds for arbitrary $\\boldsymbol{b}$ and we can therefore conclude that\n",
    "the matrix $M$ we obtain has to be equal to $A^{-1}$.\n",
    "\n",
    "We have thus derived an algorithm to actually invert the matrix $A$: \n",
    "\n",
    "our algorithm is to form the augmented equation with the full identity matrix in the place of the vector $\\boldsymbol{b}$, i.e. $[\\,A\\,|\\,I\\,]$, and perform row operations until $A$ is transformed into the identity matrix $I$, then we are left with the inverse of $A$ in the original $I$ location, i.e.\n",
    "\n",
    "$$ [\\,A\\,|\\,I\\,] \\longrightarrow [\\,I\\,|\\,A^{-1}\\,]. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update our codes from the lecture to implement a method that constructs inverse matrix through row operations via the procedure described above.\n",
    "\n",
    "Check your results against the inverse matrix given by SciPy's inverse function using the matrix\n",
    "\n",
    "```Python\n",
    "A = np.array([[2., 3., -4.], [3., -1., 2.], [4., 2., 2.]])\n",
    "```\n",
    "\n",
    "Hint: Once you have performed Gaussian elimination to transform $A$ into an upper triangular matrix, you can perform another elimination \"from bottom to top\" to transform $A$ into a diagonal matrix - you will thus need to update `upper_triangle` so that it takes in a matrix and places this on the RHS of the augmented system, and also write a similar `lower_triangle` function to perform a similar elimination starting from the last row and eliminating all entries above the main diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1304347826086956  0.3043478260869565 -0.0434782608695652]\n",
      " [-0.0434782608695653 -0.4347826086956522  0.3478260869565218]\n",
      " [-0.2173913043478261 -0.1739130434782609  0.2391304347826087]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Jordan(A):\n",
    "    \"\"\" A function to covert A into upper triangluar form through row operations.\n",
    "    The same row operations are performed on the vector b.\n",
    "    \n",
    "    Note that this implementation does not use partial pivoting which is introduced below.\n",
    "    \n",
    "    Also note that A and b are overwritten, and hence we do not need to return anything\n",
    "    from the function.\n",
    "    \"\"\"\n",
    "    \n",
    "    rows, cols = np.shape(A)\n",
    "    n = rows\n",
    "    # check A is square\n",
    "    assert(rows == cols)\n",
    "\n",
    "    \n",
    "    I = np.eye(rows,cols)\n",
    "    A = np.hstack((A,I))\n",
    "    # Loop over each pivot row - all but the last row which we will never need to use as a pivot\n",
    "    for k in range(n-1):\n",
    "        # Loop over each row below the pivot row, including the last row which we do need to update\n",
    "        for i in range(k+1, n):\n",
    "            s = (A[i, k] / A[k, k])\n",
    "            A[i, :] = A[i, :] - s*A[k, :]\n",
    "\n",
    "        for i in range(-k-2, -n-1, -1):\n",
    "            s = (A[i, cols-k-1] / A[-k-1, cols-k-1])\n",
    "            A[i, :] = A[i, :] - s*A[-k-1, :]\n",
    "        \n",
    "    for i in range(n):\n",
    "        A[i,:] /= A[i,i]\n",
    "    inv_A = A[:, cols:]\n",
    "    \n",
    "    return inv_A\n",
    "\n",
    "A = np.array([[2., 3., -4.], [3., -1., 2.], [4., 2., 2.]])\n",
    "inv_A = Jordan(A)\n",
    "print(inv_A)\n",
    "np.allclose(inv_A, sl.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - the Thomas algorithm for solving tridiagonal systems  [$\\star$]\n",
    "\n",
    "As we will see in later lectures when solving PDEs (and BVPs - basically something with a spatial derivative) it is common to find ourselves having to solve [*tridiagonal systems*](https://en.wikipedia.org/wiki/Tridiagonal_matrix).\n",
    "\n",
    "The Thomas algorithm can be considered as a simplified/specialised case of Gaussian elimination for this type of problem which has complexity $\\mathcal{O}(n)$ rather than the $\\mathcal{O}(n^3)$ required for general Gaussian elimination.\n",
    "\n",
    "A description of the algorithm is given here \n",
    "<a href=\"https://www.cfd-online.com/Wiki/Tridiagonal_matrix_algorithm_-_TDMA_(Thomas_algorithm)\">https://www.cfd-online.com/Wiki/Tridiagonal_matrix_algorithm_-_TDMA_(Thomas_algorithm)</a> \n",
    "and here\n",
    "[https://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm](https://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm).\n",
    "\n",
    "[My sample solution will use the notation and algorithm as presented at the first of these links.]\n",
    "\n",
    "Implement the algorithm and apply it to the tridiagonal matrix system given by \n",
    "\n",
    "```Python\n",
    "n = 50\n",
    "main_diag = np.ones(n)  \n",
    "off_diag = np.random.random(n-1)\n",
    "A = np.diag(-2*main_diag, 0) + np.diag(1.*off_diag, 1) + np.diag(1.*off_diag, -1)\n",
    "b = np.random.random(A.shape[0])\n",
    "```\n",
    "\n",
    "you can compare your solution against that obtained with SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "assignment destination is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b87ae3d424ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mThomas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-b87ae3d424ef>\u001b[0m in \u001b[0;36mThomas\u001b[0;34m(A, b)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: assignment destination is read-only"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "main_diag = np.ones(n)  \n",
    "off_diag = np.random.random(n-1)\n",
    "A = np.diag(-2*main_diag, 0) + np.diag(1.*off_diag, 1) + np.diag(1.*off_diag, -1)\n",
    "b = np.random.random(A.shape[0])\n",
    "\n",
    "def Thomas(A, b):\n",
    "    rows, cols = A.shape\n",
    "    n = b.size\n",
    "    \n",
    "    d = b.copy()\n",
    "    a = np.diag(A, -1)\n",
    "    b = np.diag(A)\n",
    "    c = np.diag(A, 1)\n",
    "    x = np.zeros(n)\n",
    "    \n",
    "    for k in range(n-1):\n",
    "        m = b[k] / a[k]\n",
    "        b[k+1] = b[k+1] - m * c[k]\n",
    "        d[k+1] = d[k+1] - m * d[k]\n",
    "        \n",
    "    x[-1] = d[-1] / b[-1]\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        k = -k-2\n",
    "        x[k] = (d[k] - c[k+1] * x[k+1]) / b[k]\n",
    "    \n",
    "    return x\n",
    "Thomas(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - round off error example (Gauss-Jordan inversion)\n",
    "\n",
    "In the lecture we saw an issue with round off error with the LU algorithm and the matrix \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "\\epsilon & 1 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Write a function `def Gauss_Jordan_inversion(A):` which inverts a matrix $A$ using appropriate calls to back and then forward substitution.\n",
    "\n",
    "Compare your result for the inverse matrix with that obtained from SciPy for increasingly small values of $\\epsilon$, e.g. down to $10^{-16}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jordan(A, d):\n",
    "    \"\"\" A function to covert A into upper triangluar form through row operations.\n",
    "    The same row operations are performed on the vector b.\n",
    "    \n",
    "    Note that this implementation does not use partial pivoting which is introduced below.\n",
    "    \n",
    "    Also note that A and b are overwritten, and hence we do not need to return anything\n",
    "    from the function.\n",
    "    \"\"\"\n",
    "    \n",
    "    rows, cols = np.shape(A)\n",
    "    n = rows\n",
    "    # check A is square\n",
    "    assert(rows == cols)\n",
    "\n",
    "    \n",
    "    I = np.eye(rows,cols)\n",
    "    A = np.hstack((A,I))\n",
    "    # Loop over each pivot row - all but the last row which we will never need to use as a pivot\n",
    "    for k in range(n-1):\n",
    "        # Loop over each row below the pivot row, including the last row which we do need to update\n",
    "        for i in range(k+1, n):\n",
    "            s = np.around((A[i, k] / A[k, k]),decimals=d)\n",
    "            A[i, :] = np.around(A[i, :] - s*A[k, :],decimals=d)\n",
    "\n",
    "        for i in range(-k-2, -n-1, -1):\n",
    "            s = np.around((A[i, cols-k-1] / A[-k-1, cols-k-1]),decimals=d)\n",
    "            A[i, :] = np.around(A[i, :] - s*A[-k-1, :],decimals=d)\n",
    "        \n",
    "    for i in range(n):\n",
    "        A[i,:] = np.around(A[i,:] - A[i,i], decimals=d)\n",
    "    inv_A = A[:, cols:]\n",
    "    \n",
    "    return inv_A\n",
    "\n",
    "precision = [-16,-10,-6,-4,-3,-2]*(-1)\n",
    "A = np.ones((2,2))\n",
    "for pre in precision:\n",
    "    A[0,0] = pre\n",
    "    inv_A = Jordan(A,pre)\n",
    "    print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Iterative method 3 - the Gauss-Seidel method\n",
    "\n",
    "\n",
    "We can make a small improvement to Jacobi's method using the updated components of the solution vector as soon as they become available:\n",
    "\n",
    "* Starting from a guess at the solution $\\boldsymbol{x}^{(0)}$\n",
    "\n",
    "\n",
    "* iterate for $k>0$\n",
    "$$x_i^{(k)} = \\frac{1}{a_{ii}}\\left(b_i- \\sum_{\\substack{j=1\\\\ j< i}}^n a_{ij}x_j^{(k)} - \\sum_{\\substack{j=1\\\\ j> i}}^n a_{ij}x_j^{(k-1)}\\right),\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "\n",
    "Note that as opposed to Jacobi, we can overwrite the entries of $\\boldsymbol{x}$ as they are updated; with Jacobi we need to store both the new as well as the old iteration (i.e. not overwrite the old entries until we have finished with them - which was not until the end of every iteration).\n",
    "\n",
    "As we are using updated knowledge immediately, the Gauss-Seidel algorithm should converge faster than Jacobi, but note that this convergence can only be *guaranteed* for matrices which are diagonally dominant (for every row, the magnitude of value on the main diagonal is greater than the sum of the magnitudes of all the other entries in that row), or if the matrix is *symmetric positive definite*.  \n",
    "\n",
    "Generalise the Jacobi code to solve the matrix problem using Gauss-Seidel's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1634081569933807 -0.0153270560471975  0.273352641983747\n",
      "  0.3689355530185866]\n",
      "[-0.1634081583393676 -0.0153270576876659  0.2733526430123099\n",
      "  0.3689355539464156]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Seidel(A, b, tol, maxiter):\n",
    "    n = b.size\n",
    "    x_new = np.zeros(n)\n",
    "    x = np.zeros(n)\n",
    "    for k in range(maxiter):\n",
    "        for i in range(n):\n",
    "            x_new[i] = (b[i] - A[i,:i] @ x_new[:i] - A[i, i+1:] @ x[i+1:]) / A[i,i]\n",
    "        residual = np.linalg.norm(A @ x_new - b)\n",
    "        if residual < tol:\n",
    "            break\n",
    "        x = x_new.copy()\n",
    "    \n",
    "    return x, k\n",
    "\n",
    "A = np.array([[10., 2., 3., 5.],\n",
    "              [1., 14., 6., 2.],\n",
    "              [-1., 4., 16., -4],\n",
    "              [5., 4., 3., 11.]])\n",
    "b = np.array([1., 2., 3., 4.])\n",
    "\n",
    "tol = 1e-8\n",
    "maxiter = 10000\n",
    "x_f, k = Seidel(A, b, tol, maxiter)\n",
    "x_sci = np.linalg.solve(A,b)\n",
    "print(x_f)\n",
    "print(x_sci)\n",
    "np.allclose(x_f, x_sci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Iterative method 4 - SOR (a)\n",
    "\n",
    "Convergence of the Gauss-Seidel method can be improved by a technique known as successive over relaxation (SOR). \n",
    "\n",
    "The idea is to take the new value of $x_i$ as a weighted average of its previous value and the value predicted by the regular Gauss-Seidel iteration. \n",
    "\n",
    "The corresponding formula for the $k^{th}$ iteration of the algorithm and the $i^{th}$ row is:\n",
    "\n",
    "$$x_i^{(k)} = \\frac{\\omega}{a_{ii}}\\left(b_i- \\sum_{\\substack{j=1}}^{i-1} a_{ij}x_j^{(k)} - \\sum_{\\substack{j=i+1}}^n a_{ij}x_j^{(k-1)}\\right) + (1-\\omega)x_i^{(k-1)},\\;\\;\\;\\;  i=1,2,\\ldots, n.$$\n",
    "\n",
    "where the weight $\\omega$ is called the *relaxation factor* and is usually positive. Note that when $\\omega > 1$, the method is called *over-relaxation*, whereas values  $0 < \\omega < 1$ lead to under-relaxation. This section and the method has the title S**O**R since it is the careful choice of $\\omega > 1$ that accelerates convergence.\n",
    "\n",
    "\n",
    "- What does the algorithm give for $\\omega = 0$, for $\\omega = 1$, for $0 < \\omega < 1$ ? \n",
    "\n",
    "\n",
    "- Write a function that solves a system with the relaxed Gauss-Seidel's algorithm, for a given $\\omega$.\n",
    "\n",
    "\n",
    "- Use this function to solve the system from Lecture 7,  for different values of $\\omega$. How many iterations are necessary to reach a tolerance of $10^{-6}$ for each value of $\\omega$ ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1634081588517901 -0.0153270571239354  0.2733526430756789\n",
      "  0.3689355536607956]\n",
      "41\n",
      "[-0.1634081583393676 -0.0153270576876659  0.2733526430123099\n",
      "  0.3689355539464156]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SOR(A, b, tol, maxiter, w):\n",
    "    n = b.size\n",
    "    x_new = np.zeros(n)\n",
    "    x = np.zeros(n)\n",
    "    for k in range(maxiter):\n",
    "        for i in range(n):\n",
    "            x_new[i] = (b[i] - A[i,:i] @ x_new[:i] - A[i, i+1:] @ x[i+1:]) * w / A[i,i] + (1 - w)*x[i]\n",
    "        residual = np.linalg.norm(A @ x_new - b)\n",
    "        if residual < tol:\n",
    "            break\n",
    "        x = x_new.copy()\n",
    "    \n",
    "    return x, k\n",
    "\n",
    "A = np.array([[10., 2., 3., 5.],\n",
    "              [1., 14., 6., 2.],\n",
    "              [-1., 4., 16., -4],\n",
    "              [5., 4., 3., 11.]])\n",
    "b = np.array([1., 2., 3., 4.])\n",
    "\n",
    "tol = 1e-8\n",
    "maxiter = 10000\n",
    "w = 1.5\n",
    "x_f, k = SOR(A, b, tol, maxiter, w)\n",
    "x_sci = np.linalg.solve(A,b)\n",
    "print(x_f)\n",
    "print(k)\n",
    "print(x_sci)\n",
    "np.allclose(x_f, x_sci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Iterative method 4 - SOR (b)  [$\\star$]\n",
    "\n",
    "$\\omega$ cannot be determined beforehand for an arbitrary system, \n",
    "however, an estimate can be computed during run time. \n",
    "\n",
    "Let $\\Delta_x^{(k)} = | x^{(k)} - x^{(k-1)} |$ be the magnitude of the change in x during the $k^{th}$ iteration. \n",
    "If $k$ is sufficiently large (say $k \\geq 5$), it can be shown that an approximation of the optimal value of $\\omega$ is:\n",
    "$$\n",
    "\\omega_{opt} \\approx \\frac{2}{1+\\sqrt{1-\\Delta x^{(k+1)} / \\Delta x^{(k)}}} \\,.\n",
    "$$\n",
    "\n",
    "The relaxed Gauss-Seidel algorithm can be summarised as follows:  \n",
    "\n",
    "- Carry out $k$ iterations with $\\omega = 1$ (usually $k=10$ for big systems)  \n",
    "\n",
    "\n",
    "- Record \t$\\Delta x^{(k)}$  \n",
    "\n",
    "\n",
    "- Perform an additional iteration  \n",
    "\n",
    "\n",
    "- Record \t$\\Delta x^{(k+1)}$  \n",
    "\n",
    "\n",
    "- Compute $\\omega_{opt}$  \n",
    "\n",
    "\n",
    "- Perform all subsequent iterations with $\\omega = \\omega_{opt}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Modify previous function to compute automatically the relaxation parameter $\\omega$. Compute $\\omega_{opt}$ after $k=6$ iterations as the system is small.\n",
    " \n",
    "Solve the previous system with this new function. What is the value of $\\omega$ ? How many iterations are necessary to reach a tolerance of $10^{-6}$ ?\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1634081569933807 -0.0153270560471975  0.273352641983747\n",
      "  0.3689355530185866]\n",
      "18\n",
      "[-0.1634081583393676 -0.0153270576876659  0.2733526430123099\n",
      "  0.3689355539464156]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SOR_b(A, b, tol, maxiter, w):\n",
    "    n = b.size\n",
    "    x_new = np.zeros(n)\n",
    "    x = np.zeros(n)\n",
    "    for k in range(maxiter):\n",
    "        for i in range(n):\n",
    "            x_new[i] = (b[i] - A[i,:i] @ x_new[:i] - A[i, i+1:] @ x[i+1:]) * w / A[i,i] + (1 - w)*x[i]\n",
    "        residual = np.linalg.norm(A @ x_new - b)\n",
    "        if residual < tol:\n",
    "            break\n",
    "        x = x_new.copy()\n",
    "    \n",
    "    return x, k\n",
    "\n",
    "A = np.array([[10., 2., 3., 5.],\n",
    "              [1., 14., 6., 2.],\n",
    "              [-1., 4., 16., -4],\n",
    "              [5., 4., 3., 11.]])\n",
    "b = np.array([1., 2., 3., 4.])\n",
    "\n",
    "tol = 1e-8\n",
    "maxiter = 10000\n",
    "w = 1.\n",
    "x_f, k = SOR_b(A, b, tol, maxiter, w)\n",
    "x_sci = np.linalg.solve(A,b)\n",
    "print(x_f)\n",
    "print(k)\n",
    "print(x_sci)\n",
    "np.allclose(x_f, x_sci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Algorithm comparison  [$\\star$]\n",
    "\n",
    "Let's consider $A\\boldsymbol{x}=\\boldsymbol{b}$ where:\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "5 & -2 & 0 & 0 & \\cdots & 0 \\\\\n",
    "-2 & 5 & -2 & 0 & \\cdots & 0 \\\\\n",
    "0 & -2 & 5 & -2 & \\cdots & 0 \\\\\n",
    "\\vdots & & & \\ddots & & \\vdots \\\\ \n",
    " & & & & 5 & -2 \\\\\n",
    "0 & \\cdots & & & -2 & 5 \\\\ \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "b = \\left(0, 0, \\ldots, 0, 1000  \\right)^T\n",
    "$$\n",
    "\n",
    " - Solve $A\\boldsymbol{x}=\\boldsymbol{b}$ using the relaxed Gauss-Seidel algorithm for $n=3000$. Compare the number of iterations with the algorithm without relaxation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3000\n",
    "A = np.diag(np.ones(n)*5) + np.diag(np.ones(n-1)*(-2), 1) + np.diag(np.ones(n-1)*(-2), -1)\n",
    "b = np.zeros(n)\n",
    "b[-1] = 1000\n",
    "\n",
    "tol = 1e-8\n",
    "maxiter = 10000\n",
    "w = 1\n",
    "x_f, k1 = SOR(A, b, tol, maxiter, w)\n",
    "x_wihout, k2 = Seidel(A, b, tol, maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Cramer's rule [$\\star\\star$]\n",
    "\n",
    "Write a recursive function to calculate the determinant by expanding out the first row of a matrix, using the formulae here for example: <https://en.wikipedia.org/wiki/Determinant>\n",
    "\n",
    "Hint: you may find  `np.r_` or `np.concatenate` useful to construct the indices required to form a minor of the matrix - a key line of my solution for example reads\n",
    "               \n",
    "```python\n",
    "def determinant(A):\n",
    ".\n",
    ".\n",
    ".\n",
    "    det += (-1.)**col * A[0, col] * determinant( A[1:, np.r_[0:col,col+1:n]] )\n",
    "```\n",
    "\n",
    "Use this determinant function to implement Cramer's rule: [https://en.wikipedia.org/wiki/Cramer%27s_rule](https://en.wikipedia.org/wiki/Cramer%27s_rule)\n",
    "\n",
    "and test on the system from lectures:\n",
    "\n",
    "```python\n",
    "A = np.array([[2.,3.,-4.], [6.,8.,2.], [4.,8.,-6.]])\n",
    "b = np.array([5.,3.,19.])\n",
    "```\n",
    "\n",
    "You can also compare timings for the different solvers we have encountered (our `gaussian_elimination` function, as well as SciPy's use of `inv` and `solve`), e.g. I considered the matrix\n",
    "\n",
    "```Python\n",
    "A_big = 5*np.eye(n)\n",
    "for i in range(n-1):\n",
    "    A_big[i,i+1] = -2.\n",
    "    A_big[i+1,i] = -2.\n",
    "b_big = np.zeros(n)\n",
    "b_big[n-1] = 1000.\n",
    "```\n",
    "and used something like this to compute a timing\n",
    "```Python\n",
    "tc = %timeit -n 10 -r 3 -o xc=cramers_rule(A_big,b_big)\n",
    "```\n",
    "Check the `timeit` docs to see what this does. You can extract the best time using `tc.best`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Interpolation (Vandermonde & Newton) matrices  [$\\star$]\n",
    "\n",
    "We recalled at the start of this lecture how polynomial interpolation (Lecture 1) using a monomial basis involved solution of a linear system with a Vandermonde matrix.\n",
    "\n",
    "A potential issue is that Vandermonde matrices are known to be ill-conditioned (we saw this in ACSE-2).\n",
    "\n",
    "Write some code to construct the Vandermonde matrix and investigate its condition numbers for a uniform mesh of data points in $x$ (i.e. `x = np.linspace(0, 1, n)` as a function of the number of points (`n`).\n",
    "\n",
    "\n",
    "In lecture 1 we also considered alternative basis functions and went through a derivation of the Newton polynomial (we noted that it was easier to implement than the Lagrange polynomial) - we should be able to recognise now that our derivation was essentially conducting back substitution based on a particular lower-triangular matrix!\n",
    "\n",
    "For details see:\n",
    "https://en.wikipedia.org/wiki/Newton_polynomial#Main_idea\n",
    "\n",
    "Write some code to construct the 'Newton matrix' described at this web link, and as for the Vandermonde matrix investigate its condition number for a uniform mesh of data points in $x$ as a function of the number of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Hilbert matrix  [$\\star$]\n",
    "\n",
    "NB. this is a repeat of a homework question from L4 of ACSE-2.\n",
    "\n",
    "The *Hilbert matrix* is a classic example of ill-conditioned matrix:\n",
    "\n",
    "$$\n",
    "A = \n",
    "  \\begin{pmatrix}\n",
    "    1      & 1/2    & 1/3    & \\cdots \\\\\n",
    "    1/2    & 1/3    & 1/4    & \\cdots \\\\\n",
    "    1/3    & 1/4    & 1/5    & \\cdots \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots  \\\\\n",
    "\\end{pmatrix}\\,.\n",
    "$$\n",
    "\n",
    "Let's consider the linear system $A\\boldsymbol{x}=\\boldsymbol{b}$ where \n",
    "\n",
    "$$ b_i = \\sum_{j=1}^n a_{ij},\\;\\;\\; \\text{for}\\;\\;\\;\\; i=1,2,\\ldots, n.$$\n",
    "\n",
    "- How can you write entry $A_{ij}$ for any $i$ and $j$ ?\n",
    "\n",
    "\n",
    "- Convince yourself by pen and paper that $ \\boldsymbol{x} = \\left[ 1, 1, \\cdots 1\\right]^T$ is the solution of the system.\n",
    "\n",
    "\n",
    "- Write a function that returns $A$ and $b$ for a given $n$.\n",
    "\n",
    "\n",
    "- For a range of $n$, compute the condition number of $A$, solve the linear system and compute the error ($err = \\sum_{i=1}^n \\left|x_{computed, i}-x_{exact, i}\\right|$). What do you observe ?\n",
    "\n",
    "\n",
    "- Hint: take a look at the 'error_vs_condition_number' figure from the lecutre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Comparison of iterative methods  [$\\star$]\n",
    "\n",
    "Now that we have implemented several iterative solvers, compare their performance (residual vs iteration number) on the following matrix\n",
    "\n",
    "```Python\n",
    "# diagonally dominant matrix (random)\n",
    "n = 1000\n",
    "main_diag = np.ones(n)  \n",
    "off_diag = np.random.random(n-1)\n",
    "A = np.diag(-2*main_diag, 0) + np.diag(1.*off_diag, 1) + np.diag(1.*off_diag, -1)\n",
    "b = np.random.random(A.shape[0])\n",
    "```\n",
    "\n",
    "Hint: the 'direct_vs_iterative' figure from the lecture may provide some motivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Cubic splines (from Lecture 1)  [$\\star\\star$]\n",
    "\n",
    "If you didn't attempt it at the time go back and look at the homework exercise from lecture 1 on the generation of a cubic spline approximation to data via the formulation and solve of a linear system."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
