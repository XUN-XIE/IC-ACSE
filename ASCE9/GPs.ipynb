{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import utide\n",
    "import datetime as d\n",
    "# import sklearn.gaussian_process.kernels as ke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(base_path):\n",
    "    name = [\"t1\", \"t2\", \"t3\", \"t4\", \"v1\", \"v2\", \"v3\", \"v4\"]\n",
    "    data = []\n",
    "    for i in name:\n",
    "        path = base_path + i + \".mat\"\n",
    "        tmp = loadmat(path)\n",
    "        tmp_data = tmp[i]\n",
    "        \n",
    "        data.append(tmp_data.reshape(-1))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locations(node_number):\n",
    "    f = open(\"./Data/fort.14\")\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "\n",
    "    all_data = []\n",
    "    for i in range(5200):\n",
    "        read_txt = f.readline().split()\n",
    "        for i in range(4):\n",
    "            read_txt[i] = float(read_txt[i])\n",
    "\n",
    "        all_data.append(read_txt)\n",
    "    \n",
    "    # returen the longitutde and latitude corresponds to the node\n",
    "    return all_data[node_number][1], all_data[node_number][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node(base_path, node_number):\n",
    "    long_node, lat_node = get_locations(node_number)\n",
    "    \n",
    "    data = read_data(base_path)\n",
    "    \n",
    "    u_node = np.array([])\n",
    "    v_node = np.array([])\n",
    "    for i in range(4):\n",
    "        u_node = np.concatenate((u_node, data[4+i][192:].real))\n",
    "        v_node = np.concatenate((v_node, data[4+i][192:].imag))\n",
    "    \n",
    "    for i in range(4):\n",
    "        data[i] = data[i] / 86400\n",
    "        \n",
    "    # 直接计算四个日期间用小数表示的形式直接进行计算！！！后期整合再全部使用datetime\n",
    "    \n",
    "    t1 = (data[0][192:])\n",
    "    t2 = (data[1][192:] + (np.datetime64('2014-02-28') - np.datetime64('2013-12-30')).astype(int))\n",
    "    t3 = (data[2][192:] + (np.datetime64('2014-04-29') - np.datetime64('2013-12-30')).astype(int))\n",
    "    t4 = (data[3][192:] + (np.datetime64('2014-06-28') - np.datetime64('2013-12-30')).astype(int))\n",
    "    \n",
    "    \n",
    "    \n",
    "    t_raw_node = np.concatenate((t1,t2,t3,t4))\n",
    "    u_raw_node = u_node\n",
    "    v_raw_node = v_node\n",
    "    \n",
    "    return t_raw_node, u_raw_node, v_raw_node, long_node, lat_node\n",
    "\n",
    "# t,u,v,lo,la = node('./Data/1/', 4043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computes the constituent frquencies for the time length of the data and sampling frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = [4034, 3832, 3719, 3721, 3839, 3947, 4048, 4045, 3942, 3939, 4038, 3833, 3836, 3944, 3941]\n",
    "t_raw_node, u_raw_node, v_raw_node, long_node, lat_node = [], [], [], [], []\n",
    "for i in range(15):\n",
    "    path = './Data/' + str(i+1) + '/'\n",
    "    t,u,v,lo,la = node(path, node_list[i])\n",
    "    t_raw_node.append(t)\n",
    "    u_raw_node.append(u)\n",
    "    v_raw_node.append(v)\n",
    "    long_node.append(lo)\n",
    "    lat_node.append(la)\n",
    "\n",
    "\n",
    "t_raw_node_reduced, u_raw_node_reduced, v_raw_node_reduced = t_raw_node[0][::4], u_raw_node[0][::4], v_raw_node[0][::4]\n",
    "for i in range(14):\n",
    "    t_raw_node_reduced = np.vstack((t_raw_node_reduced, t_raw_node[i+1][::4]))\n",
    "    u_raw_node_reduced = np.vstack((u_raw_node_reduced, u_raw_node[i+1][::4]))\n",
    "    v_raw_node_reduced = np.vstack((v_raw_node_reduced, v_raw_node[i+1][::4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solve: matrix prep ... solution ... diagnostics ... done.\n"
     ]
    }
   ],
   "source": [
    "tr_start = 0\n",
    "tr_end = 2184\n",
    "test_start = 2185\n",
    "test_end = 2184*2 + 1\n",
    "\n",
    "t_raw_utide, u_raw_utide, v_raw_utide = t_raw_node_reduced[4, tr_start:tr_end], u_raw_node_reduced[4, tr_start:tr_end], v_raw_node_reduced[4, tr_start:tr_end]\n",
    "\n",
    "lat = lat_node[0]\n",
    "\n",
    "coef = utide.solve(t_raw_utide, u_raw_utide, v_raw_utide, lat=lat, method='ols')\n",
    "A1 = coef.name\n",
    "\n",
    "num_const = len(A1)\n",
    "\n",
    "ut = loadmat(\"./UTide/ut_constants.mat\")\n",
    "\n",
    "A = ut['const'][0][0][0]\n",
    "freq = ut[\"const\"][0][0][1]\n",
    "\n",
    "selected_const_index = []  # location/index of the selected constituent in the list of all constituents\n",
    "\n",
    "for j in A1:\n",
    "    index = 0\n",
    "    length = len(j)\n",
    "    for k in A:\n",
    "        if j == k[:length]:\n",
    "            selected_const_index.append(index)\n",
    "            break\n",
    "        index = index + 1\n",
    "\n",
    "selected_const_freq = freq[selected_const_index].reshape(-1) # frequency of the selected constituents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert  latitude and longitude of the locations to spatial distances from a mean location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_node = np.array(lat_node)\n",
    "long_node = np.array(long_node)\n",
    "\n",
    "mean_lat, mean_long = np.mean(lat_node), np.mean(long_node)\n",
    "lat0, long0 = np.min(lat_node), np.min(long_node)\n",
    "\n",
    "num_location = 15\n",
    "dlong, dlat = [], []\n",
    "for i in range(num_location):\n",
    "    theta1 = lat_node[i]\n",
    "    theta2 = lat_node[i]\n",
    "    lambda1 = mean_long\n",
    "    lambda2 = long_node[i]\n",
    "    \n",
    "    tmp_long = 6371 * np.arccos( np.sin( theta1 / 180 * np.pi ) * np.sin( theta2 / 180 * np.pi ) + np.cos( theta1 / 180 * np.pi) * np.cos( theta2 / 180 * np.pi) * np.cos( ( lambda2 - lambda1 ) / 180 * np.pi) )\n",
    "    \n",
    "    theta1 = mean_lat\n",
    "    theta2 = lat_node[i]\n",
    "    lambda1 = long_node[i]\n",
    "    lambda2 = long_node[i]\n",
    "    \n",
    "    tmp_lat = 6371 * np.arccos( np.sin( theta1 / 180 * np.pi ) * np.sin( theta2 / 180 * np.pi ) + np.cos( theta1 / 180 * np.pi) * np.cos( theta2 / 180 * np.pi) * np.cos( ( lambda2 - lambda1 ) / 180 * np.pi) )\n",
    "    \n",
    "    dlong.append(tmp_long)\n",
    "    dlat.append(tmp_lat)\n",
    "\n",
    "dlong = np.array(dlong)\n",
    "dlat = np.array(dlat)\n",
    "\n",
    "for i in range(num_location):\n",
    "    if long_node[i] < mean_long:\n",
    "        dlong[i] = - dlong[i]\n",
    "    if lat_node[i] < mean_lat:\n",
    "        dlat[i] = - dlat[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the inputs x and outputs y for training the Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts=int(168/(7*24))\n",
    "combn = loadmat(\"./sampling/combn_hour.mat\")[\"combn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t1 = np.diag(t_raw_node_reduced[combn[:,0]-1])\n",
    "x_t2 = np.diag(t_raw_node_reduced[combn[:,1]-1])\n",
    "\n",
    "x_dlat1 = dlat[combn[:,0]-1]\n",
    "x_dlat2 = dlat[combn[:,1]-1]\n",
    "\n",
    "x_dlong1 = dlong[combn[:,0]-1]\n",
    "x_dlong2 = dlong[combn[:,1]-1]\n",
    "\n",
    "num = len(x_t1)+len(x_t2)\n",
    "x_t = np.concatenate((x_t1, x_t2)).reshape(num, 1)\n",
    "x_dlat = np.concatenate((x_dlat1, x_dlat2)).reshape(num, 1)\n",
    "x_dlong = np.concatenate((x_dlong1, x_dlong2)).reshape(num, 1)\n",
    "\n",
    "x1 = np.hstack((x_t, x_dlat, x_dlong))\n",
    "\n",
    "y_u1 = np.diag(u_raw_node_reduced[combn[:,0]-1])\n",
    "y_u2 = np.diag(u_raw_node_reduced[combn[:,1]-1])\n",
    "\n",
    "y1a = np.concatenate((y_u1, y_u1)).reshape(num,1)\n",
    "\n",
    "y_v1 = np.diag(v_raw_node_reduced[combn[:,0]-1])\n",
    "y_v2 = np.diag(v_raw_node_reduced[combn[:,1]-1])\n",
    "\n",
    "y1b = np.concatenate((y_v1, y_v1)).reshape(num,1)\n",
    "\n",
    "t_specific = np.vstack((x_t1, x_t2)).T\n",
    "u_specific = np.vstack((y_u1, y_u2)).T\n",
    "v_specific = np.vstack((y_v1, y_v2)).T\n",
    "\n",
    "y1 = np.hstack((y1a, y1b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
