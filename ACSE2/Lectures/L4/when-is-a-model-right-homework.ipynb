{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACSE-2  <a class=\"tocSkip\">\n",
    "\n",
    "## When is a \"model\" right, how do we demonstrate this, and how can things go wrong?   <a class=\"tocSkip\">\n",
    "    \n",
    "### Homework Exercises <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Homework\" data-toc-modified-id=\"Homework-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Homework</a></span><ul class=\"toc-item\"><li><span><a href=\"#Homework---$\\ell^p$-norms\" data-toc-modified-id=\"Homework---$\\ell^p$-norms-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Homework - $\\ell^p$ norms</a></span></li><li><span><a href=\"#Homework---norms-in-linear-best-fit-[*]\" data-toc-modified-id=\"Homework---norms-in-linear-best-fit-[*]-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Homework - norms in linear best fit [*]</a></span></li><li><span><a href=\"#Homework---SymPy-and-the-logistic-ODE\" data-toc-modified-id=\"Homework---SymPy-and-the-logistic-ODE-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Homework - SymPy and the logistic ODE</a></span></li><li><span><a href=\"#Homework---round-off-vs-truncation-errors\" data-toc-modified-id=\"Homework---round-off-vs-truncation-errors-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Homework - round-off vs truncation errors</a></span></li><li><span><a href=\"#Homework---Vandermonde-matrices-[*]\" data-toc-modified-id=\"Homework---Vandermonde-matrices-[*]-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Homework - Vandermonde matrices [*]</a></span></li><li><span><a href=\"#Homework---ill-conditioned-matrices-and-the-solution-of-corresponding-linear-systems-[*]\" data-toc-modified-id=\"Homework---ill-conditioned-matrices-and-the-solution-of-corresponding-linear-systems-[*]-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Homework - ill-conditioned matrices and the solution of corresponding linear systems [*]</a></span></li><li><span><a href=\"#Homework---Hilbert-matrix-[*]\" data-toc-modified-id=\"Homework---Hilbert-matrix-[*]-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Homework - Hilbert matrix [*]</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%precision 16\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - $\\ell^p$ norms\n",
    "\n",
    "See <https://en.wikipedia.org/wiki/Sequence_space#%E2%84%93p_spaces>\n",
    "\n",
    "Recall our vector norms from lecture\n",
    "\n",
    "\\begin{align*}\n",
    "\\|\\boldsymbol{v}\\,\\|_2 & = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2} = \\left(\\sum_{i=1}^n v_i^2 \\right)^{1/2}, &&{\\text{the two-norm}}\\\\[5pt]\n",
    "\\|\\boldsymbol{v}\\,\\|_1  & = |v_1| + |v_2| + \\ldots + |v_n| = \\sum_{i=1}^n |v_i|, &&{\\text{the one-norm}}\\\\[5pt]\n",
    "\\|\\boldsymbol{v}\\,\\|_{\\infty}  &= \\max\\{|v_1|,|v_2|, \\ldots, |v_n| = \\max_{i=1}^n |v_i|, &&{\\text{the max-norm}}\n",
    "\\end{align*}\n",
    "\n",
    "we can generalise the \"two-norm\" to the \"$p$-norm\":\n",
    "\n",
    "$$\\|\\boldsymbol{v}\\,\\|_p = \\left(v_1^2 + v_2^2 + \\ldots + v_n^2\\right)^{1/p} \n",
    "= \\left(\\sum_{i=1}^n v_i^p \\right)^{1/p}$$\n",
    "\n",
    "Plot the unit circle in terms of this norm - what shapes does it recreate for $p=1$ and $p$ very large?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - norms in linear best fit [*]\n",
    "\n",
    "Try updating the example from class so that the linear best fit line goes through the origin, i.e. you are optimising for the slope parameter only with each line going through the origin (the intercept is zero).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - SymPy and the logistic ODE\n",
    "\n",
    "\n",
    "In lecture 1 we considered the logistic ODE\n",
    "\n",
    "$$\\frac{dx}{dt} = \\alpha x(1-x), \\qquad x(0)=x_0$$\n",
    "\n",
    "and derived the following solution\n",
    "\n",
    "$$x(t) = \\frac{x_0}{x_0 + (1-x_0)e^{-\\alpha t}}$$\n",
    "\n",
    "Use SymPy to derive the same solution symbolically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - round-off vs truncation errors\n",
    "\n",
    "Given a function $f(x)$, a Taylor series analysis tells us that the following is a first-order approximation to its derivative:\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x+\\Delta x) - f(x)}{\\Delta x} $$\n",
    "\n",
    "Of course this is just the finite version of the mathematics you used to first learn about what a derivative is:\n",
    "\n",
    "$$f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x+\\Delta x) - f(x)}{\\Delta x} $$\n",
    "\n",
    "Code up this approximation and plot the error as a function of $\\Delta x$, and see what happens as we vary $\\Delta x$ from very large to very small values (make sure you don't go to extreme with these as you might experience overflow!)\n",
    "\n",
    "You could test with the function $f(x) = \\exp(x)$ at $x=1$ say, but other examples should show similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Vandermonde matrices [*]\n",
    "\n",
    "We will see in ACSE-3 how polynomial interpolation using a monomial basis involves the solution of a linear system with a so-called [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix).\n",
    "\n",
    "A potential issue is that Vandermonde matrices are known to be ill-conditioned.\n",
    "\n",
    "Assume that we are dealing with square matrix, write some code to construct the Vandermonde matrix given a vector of $\\alpha$ values. You can check your code against `np.vander`.\n",
    "\n",
    "Investigate how the condition number of the matrix varies for a uniform mesh of data points in $x$ (i.e. `alpha = np.linspace(0, 1, n)` as a function of the number of points in the mesh (`n`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - ill-conditioned matrices and the solution of corresponding linear systems [*]\n",
    "\n",
    "Consider the problem from the lecture\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "  \\begin{array}{cc}\n",
    "    2 & 1 \\\\\n",
    "    2 & 1 + \\epsilon  \\\\\n",
    "  \\end{array}\n",
    "\\right)\\left(\n",
    "  \\begin{array}{c}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "  \\end{array}\n",
    "\\right) = \\left(\n",
    "  \\begin{array}{c}\n",
    "    3 \\\\\n",
    "    0 \\\\\n",
    "  \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "and let's consider the case where $\\epsilon = 0.1$.\n",
    "\n",
    "Consider the two extreme points we computed from class that corresponded to the magenta and black dots in our plots\n",
    "\n",
    "```Python\n",
    "A = np.array([[2.,1.],[2.,1. + 0.1]])\n",
    "\n",
    "# these are the points we computed from class\n",
    "origin_p = np.array([2.235621,2.282122])\n",
    "origin_p2 = np.array([-0.044721,0.04381])\n",
    "```\n",
    "\n",
    "construct a small (I used a perturbation size of magnitude 0.01) circle of points around each of these - consider these as perturbed RHS $\\boldsymbol{b}$ values. \n",
    "\n",
    "Compute and plot the corresponding shape you get for the set of perturbed solutions $\\boldsymbol{x}$.\n",
    "\n",
    "Compute the maximum value of the ratio of the relative perturbation in $x$ to the relative perturbation in $b$:\n",
    "\n",
    "$$\\frac{  {\\|\\delta \\boldsymbol{x}\\|} / {\\|\\boldsymbol{x}\\|}   }{  {\\|\\delta \\boldsymbol{b}\\|} / {\\|\\boldsymbol{b}\\|}    } $$\n",
    "\n",
    "for each case and convince yourself that for both this quantity is bounded by the condition number as the theory implies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Hilbert matrix [*]\n",
    "\n",
    "The *Hilbert matrix* is a classic example of ill-conditioned (square) matrix:\n",
    "\n",
    "$$\n",
    "A = \n",
    "  \\begin{pmatrix}\n",
    "    1      & 1/2    & 1/3    & \\cdots \\\\\n",
    "    1/2    & 1/3    & 1/4    & \\cdots \\\\\n",
    "    1/3    & 1/4    & 1/5    & \\cdots \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots  \\\\\n",
    "\\end{pmatrix}\\,.\n",
    "$$\n",
    "\n",
    "Let's consider the linear system $A\\boldsymbol{x}=\\boldsymbol{b}$ where the RHS vector is defined as\n",
    "\n",
    "$$ b_i = \\sum_{j=1}^n A_{ij},\\;\\;\\; \\text{for}\\;\\;\\;\\; i=1,2,\\ldots, n.$$\n",
    "\n",
    "\n",
    "- Think about how you can write entry $a_{ij}$ for any $i$ and $j$ mathematically, and use this to write a function that generated the Hilbert matrix for a given $n$.\n",
    "\n",
    "\n",
    "- Convince yourself that $ \\boldsymbol{x} = \\left[ 1, 1, \\cdots 1\\right]^T$ is the exact solution of the system.\n",
    "\n",
    "\n",
    "- Write a function that returns $A$ and $b$ for a given $n$.\n",
    "\n",
    "\n",
    "- For a range of $n$, compute the condition number of $A$, solve the linear system and compute the error between the NumPy/SciPy solution you obtain and the exact solution.  How does the error vary with $n$?\n",
    "\n",
    "<br>\n",
    "\n",
    "**Notes**\n",
    "\n",
    "For this example we do not need to introduce any explicit perturbations in our problem in order to generate errors in our solution.\n",
    "\n",
    "We can interpret the exact mathematical problem as written down above as the unperturbed case with the unperturbed exact solution being $ \\boldsymbol{x} = \\left[ 1, 1, \\cdots 1\\right]^T$.\n",
    "\n",
    "Floating point errors will mean that every case we consider numerically will be perturbed slightly - both the entries of $A$ and $\\boldsymbol{b}$ won't be exactly as they should be. With finite precision arithmetic our calculations can only be as accurate as [*machine epsilon*](https://en.wikipedia.org/wiki/Machine_epsilon) which assuming double precision is given by\n",
    "\n",
    "```Python\n",
    "print('machine epsilon = ', np.finfo(float).eps)\n",
    "machine epsilon =  2.220446049250313e-16\n",
    "```\n",
    "The relation \n",
    "\n",
    "$$ \\text{relative error of output}  \\sim  \\text{condition number} \\times  \\text{relative error of input} $$\n",
    "\n",
    "therefore gives us an upper bound on how accurate we can expect our solution to be:\n",
    "\n",
    "$$ \\text{relative error of output}  \\sim  \\text{condition number} \\times  \\text{machine epsilon}. $$\n",
    "\n",
    "Add this relation to your relative error plot and check it indeed provides an upper bound, which grows with $n$, and that our relative error is close to this value and grows in tandem with it as $n$ increases.\n",
    "\n",
    "This emphasises the point that for ill-conditioned problems large errors can be expected even if only floating point errors are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.33333333333333331\n",
      "0.33333333333333331483\n",
      "0.3333333333284827\n",
      "0.9999999999854481\n"
     ]
    }
   ],
   "source": [
    "a = 1/3\n",
    "print(a)\n",
    "print('{:.16f}'.format(a))\n",
    "print('{:.17f}'.format(a))\n",
    "print('{:.20f}'.format(a))\n",
    "b = 100000*a - 33333\n",
    "print(b)\n",
    "print(3*b)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
