{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afternoon session 2:\n",
    "### Optimization and Neural Networks\n",
    "\n",
    "Read the following paper: [An overview of gradient descent optimization\n",
    "algorithms](https://arxiv.org/pdf/1609.04747.pdf).  \n",
    "(At least read about stochastic, full-batch and mini-batch gradient descent, SGD, Momentum and Adam.)\n",
    "\n",
    "Then try to implement the following:\n",
    "1. Implement Momentum Accelerated gradient-descent for your backprop from scratch example.\n",
    "    - If you have not implemented your own version this morning, try to finish this or use the provided example code.\n",
    "    - The provided code also includes a bias term. \n",
    "    - All you have to implmement are the updates for the bias terms.\n",
    "    - Compare your results from the 1-hidden layer network with bias, with (momentum=0.9) and without momentum.\n",
    "    - Does this improve your results?\n",
    "2. Compare different optimization methods using the Ackley-Exercise\n",
    "    - Create plots for SGD, SGD+Momentum=0.9, RMSProp, Adam\n",
    "    - Compare for a number of learning rates lr=[10, 1, 1e-1, 1e-2]\n",
    "    - The final result should be a 4x4 matrix of plots of the Ackley function optimized from 100 starting locations (set_seed(42)) to get the same locations every time.\n",
    "    - What do you observe in terms of local and global minima?\n",
    "3. For the final exercise from the morning session create a network that achieves at least 98% accuracy on the test set. You may add layers, add weights, change learning rates and optimization methods. Use full-batch training.\n",
    "\n",
    "Additional Reading Material:  \n",
    "[Visualizing the Loss Landscapes of Neural Networks](https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf)  \n",
    "[On the importance of initialization and momentum in deep learning](http://proceedings.mlr.press/v28/sutskever13.pdf)  \n",
    "[Why Momentum really works distill.pub](https://distill.pub/2017/momentum/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop from Scratch with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(batch_size, batch_num, test_size, noise=0.05):\n",
    "    \"\"\"\n",
    "    Makes a two-moon train-test dataset with fixed batch size, number and noise level\n",
    "    \"\"\"\n",
    "    X_train, y_train = make_moons(n_samples=batch_size*batch_num, noise=noise)\n",
    "    y_train = y_train.reshape(batch_num, batch_size, 1)\n",
    "    X_train = X_train.reshape(batch_num, batch_size, 2)\n",
    "\n",
    "\n",
    "    X_test, y_test = make_moons(noise=0.1)\n",
    "    y_test = y_test.reshape(test_size, 1)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ackely Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the two-moons neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
