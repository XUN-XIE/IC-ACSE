{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACSE Module 8 - Practical - Morning Session 5:\n",
    "\n",
    "# Time-Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Time-Series Data?\n",
    "Any data that varies along the time dimension is called Time-Series data. e.g.:\n",
    "- Sales of ice cream per day in the Senior Common Room at Imperial College - univariate\n",
    "- Average price of the Apple stock by day - univariate\n",
    "- The closing value of FTSE100 by day - univariate\n",
    "- Average daily temperatures in Australia - univariate\n",
    "- The complete text of 1984 by George Orwell\n",
    "- EEG data - multivariate\n",
    "- Room occupancy sensor data - multivariate\n",
    "\n",
    "## What can we do with time-series?\n",
    "- Forecasting - predict the future!\n",
    "- Generate text - e.g. autocomplete\n",
    "- Anomaly detection - e.g. detect a siezure happening/about to happen from EEG data\n",
    "- Classification - e.g. room occupied/not occupied, sentiment analysis of text\n",
    "\n",
    "## What makes time-series data special?\n",
    "- There is a concept of \"past\" and \"future\" in time-series data\n",
    "- In the case of Forecasting, the model must be retrained after essentially every prediction\n",
    "- Non-stationary probability distribution of data\n",
    "\n",
    "Although there is a large number of classical time-series methods that work really well, we are going to focus on Neural Networks in this session. \n",
    "\n",
    "The neural networks we have seen so far have no concept of _order_ in inputs. Each time a fresh input is presented, it is treated in isolation by the NN architectures we have seen so far. In time-series analysis, the _context_ is extremely important, i.e. what came before this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) solve this problem by incorporating loops within them. The loop signifies that information can persist from one set of inputs to another. \n",
    "\n",
    "<img src=\"images/RNN-rolled.png\" width=200 />\n",
    "\n",
    "This network accepts input _x<sub>t</sub>_ and provides output _h<sub>t</sub>_ at every step _t_. \n",
    "\n",
    "The network can also be seen in the following _unrolled_ visualisation:\n",
    "\n",
    "<img src=\"images/RNN-unrolled.png\" width=600 />\n",
    "\n",
    "\n",
    "Simple RNNs, however, suffer from problems of _short-term memory_. \n",
    "\n",
    "![](images/LSTM.png)\n",
    "\n",
    "\n",
    "For example, try to predict the last word in the sentence: **There are so many clouds in the _sky_.**\n",
    "\n",
    "This is easy for a simple RNN to predict as the necessary context word _clouds_ appeared just two words ago. \n",
    "\n",
    "However, look at the following example: **I grew up in France... I speak fluent _French_** \n",
    "\n",
    "The distance between the contextual clue word _France_ and the predicted word _French_ could have been arbitrarily long in this text. Simple RNNs have trouble learning to connect such contextual references that appear far away from each other. \n",
    "\n",
    "[Let's see if you can differentiate between machine generated text and human written text](http://goopt2.xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data\n",
    "\n",
    "Textual data can also be seen as a time-series since it varies along a single dimension. There are two distinct ways of looking at this data - every character can be seen as an individual data point, or every word can be seen as a new data point. \n",
    "\n",
    "### Character-level\n",
    "\n",
    "When working with text data at the character level, the advantage is that the set of possible inputs (all possible characters) is relatively small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "!mkdir data\n",
    "!cd data && wget https://raw.githubusercontent.com/acse-2019/ACSE-8/master/Implementation/practical_5/morning/data/names.zip && unzip names.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "all_letters = string.ascii_letters\n",
    "n_letters = len(all_letters)\n",
    "all_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, an obvious disadvantage is the risk of _missing the forest for the trees_ , i.e. higher-level patterns might be less obvious when looking at fine-grained data. \n",
    "\n",
    "### Objective: Name-origin classifier\n",
    "Given a dataset of common last names from different languages, classify a new (previously unseen) last name into one of the (seen) languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset\n",
    "import glob\n",
    "\n",
    "all_filenames = glob.glob('data/names/*.txt')\n",
    "print(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicode_to_ascii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "for filename in all_filenames:\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "print('n_categories =', n_categories)\n",
    "total_names = sum([len(x) for x in category_lines.values()])\n",
    "print(total_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Turning names into Tensors\n",
    "\n",
    "We have loaded the names from the dataset into memory but our neural networks can only deal with Tensors so we need a tensor representation for a name. \n",
    "\n",
    "To represent each letter numerically, we could simply use its position in the alphabet, e.g. 1 for a, but this would imply an ordering between the letters - that in some way b > a. We don't want our model to learn such an ordering so we want a _non-ordinal_ representation. A common one is _one-hot encoding_. \n",
    "\n",
    "##### One-hot encoding\n",
    "\n",
    "One-hot encoding generates vectors that are the size of number of all possible outcomes, and contain zeros everywhere except one location, which has a 1. \n",
    "e.g. To represent _Gender_, we could use 1 to represent _Male_ and 2 to represent _Female_ , but this would imply that _Female_ > _Male_ . So instead we use the vector [0 1] to represent _Male_ and the vector [1 0] to represent _Female_. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write a function to convert a letter to a one-hot tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "all_letters = string.ascii_letters\n",
    "n_letters = len(all_letters)\n",
    "all_letters\n",
    "\n",
    "def letter_to_tensor(letter):\n",
    "    # Write your code below\n",
    "    # Initialize a torch tensor of zeros of appropriate length\n",
    "    # Find the location of the letter in *all_letters* above\n",
    "    # Set the appropriate location to 1 here\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def line_to_tensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        letter_index = all_letters.find(letter)\n",
    "        tensor[li][0][letter_index] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(letter_to_tensor('k'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(line_to_tensor('Kukreja').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: The Network\n",
    "![](images/RNN-network.png)\n",
    "\n",
    "This RNN module (from [here](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)) is just 2 linear layers which operate on an input and hidden state, with a LogSoftmax layer after the output. Before continuing, think about whether we want to do the classification at every step here or are we only interested in the output at the end Complete the code in the following cell to implement the above network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, i, hidden):\n",
    "        # Write your code below\n",
    "        # Combine the input and the hidden into a single tensor\n",
    "        # Do the Input->Hidden Transform\n",
    "        # Do the Input->Output Transform\n",
    "        # Softmax on Output\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test this module we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = letter_to_tensor('A')\n",
    "hidden = rnn.init_hidden()\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "print('output.size =', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = line_to_tensor('Dubrule')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_from_output(output):\n",
    "    top_value, top_index = output.data.topk(1)\n",
    "    category_index = top_index[0][0]\n",
    "    return all_categories[category_index], category_index\n",
    "\n",
    "print(category_from_output(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = line_to_tensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "lr = 0.005 \n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Why](https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/other/pytorch-lossfunc-cheatsheet.md) NLLLoss? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: The train function\n",
    "\n",
    "For the training function here, we will implement _true_ Stochastic Gradient Descent, i.e. one example at a time. \n",
    "\n",
    "The train function must:\n",
    "- Reset gradients\n",
    "- Create a zeroed initial hidden state\n",
    "- Read each letter in and\n",
    "- Keep hidden state for next letter\n",
    "- Compare final output to target\n",
    "- Back-propagate\n",
    "- Return the output and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(category_tensor, line_tensor):\n",
    "    # Write your code below\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = category_from_output(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evolution of the loss function through the training process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the loss function going down through the training iterations - the network is learning.\n",
    "\n",
    "Next, let's plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = category_from_output(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our network is doing very well with Greek, but very bad with English!\n",
    "\n",
    "Running on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    output = evaluate(Variable(line_to_tensor(input_line)))\n",
    "\n",
    "    # Get top N categories\n",
    "    topv, topi = output.data.topk(n_predictions, 1, True)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        value = topv[0][i]\n",
    "        category_index = topi[0][i]\n",
    "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "        predictions.append([value, all_categories[category_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Zhang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Name generator\n",
    "\n",
    "Now let's modify the previous example to build a name generator instead. First, some utility functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: The \"Generator\" Network\n",
    "This time we use a slightly modified version of the previous network:\n",
    "![](images/RNN-generator.png)\n",
    "\n",
    "This version has an added input for the category tensor, where we specify which origin name we would like the network to generate. This category will be another one-hot tensor like the letter input. \n",
    "\n",
    "Another change is the addition of a dropout layer before the softmax. This allows for some randomization to increase the sampling variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, input, hidden):\n",
    "        # Write your code below\n",
    "        # Combine category, input and hidden into a single tensor\n",
    "        # Input to hidden\n",
    "        # Input to Output\n",
    "        # output combined\n",
    "        # Output to output\n",
    "        # Dropout\n",
    "        # SoftMax\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each timestep (that is, for each letter in a training word) the inputs of the network will be (category, current letter, hidden state) and the outputs will be (next letter, next hidden state). So for each training set, we’ll need the category, a set of input letters, and a set of output/target letters.\n",
    "\n",
    "Since we are predicting the next letter from the current letter for each timestep, the letter pairs are groups of consecutive letters from the line - e.g. for \"ABCD<EOS>\" we would create (“A”, “B”), (“B”, “C”), (“C”, “D”), (“D”, “EOS”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot vector for category\n",
    "def categoryTensor(category):\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][all_categories.index(category)] = 1\n",
    "    return tensor\n",
    "\n",
    "categoryTensor(\"Greek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "targetTensor(\"Lombardo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random category and random line from that category\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line\n",
    "\n",
    "# Make category, input, and target tensors from a random category, line pair\n",
    "def randomTrainingExample():\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = line_to_tensor(line)\n",
    "    target_line_tensor = targetTensor(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor\n",
    "randomTrainingExample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the training function. Note that in contrast to the classification train function, we now use the output of the network at every step instead of only at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: The train function for the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, optimizer, criterion, category_tensor, input_line_tensor, target_line_tensor):\n",
    "    target_line_tensor.unsqueeze_(-1)    \n",
    "    \n",
    "    # Write your code below\n",
    "    # Initialise hidden\n",
    "    # Reset gradients\n",
    "\n",
    "    # Initialise loss\n",
    "\n",
    "    # loop over the characters in put\n",
    "    # Forward prop one letter at a time\n",
    "    # loss function w.r.t expected output (each letter)\n",
    "    # Sum losses\n",
    "\n",
    "    # Backprop\n",
    "    \n",
    "    # Gradient step\n",
    "    \n",
    "    return output, loss.item() / input_line_tensor.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "lr = 0.0005\n",
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0. # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    output, loss = train(rnn, optimizer, criterion, *randomTrainingExample())\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the losses to see the progress of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from the network\n",
    "\n",
    "Now we will write the function that generates names using this network. To sample, we feed in a category (name origin), as well as a first letter as a seed - and ask the network what the next letter is. Next, keeping the category the same, we feed in the second letter (generated by the network in the previous step) as the seed and ask for the next letter, and so on until we get the EOS (end-of-string) token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(category, start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter))\n",
    "\n",
    "samples('Russian', 'RUS')\n",
    "print(\"***\")\n",
    "samples('German', 'GER')\n",
    "print(\"***\")\n",
    "samples('Spanish', 'SPA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recent Advances\n",
    "\n",
    "The long-term memory in LSTM is [a specific instance](https://arxiv.org/pdf/1601.06733.pdf) of a more generic concept called _Attention_. The concept of Attention was introduced to solve one problem - when doing _Neural Machine Translation_ , the next word in the output sentence (in the output language) is not necessary related to the last (or second-to-last) word in the input sentence (in the input language). Since simple RNNs can only capture adjacency relationships, various styles of attention were tried to teach the model to look at a specific part of the input sentence in order to predict the next output word. [Many of these attention approaches](https://arxiv.org/abs/1409.0473) were successful and today far outperform LSTMs on the above tasks. \n",
    "\n",
    "![](images/self-attention.png)\n",
    "\n",
    "One extremely successful kind of attention is _self attention_. Here, instead of mapping relationships between an output sequence and an input sequence, we map relationships between the different words of the same sentence. Going down this path, it was realised that the self-attention mechanism is more than just an add-on to RNNs and it might be possible to build entire networks out of self-attention alone. In [\"Attention is all you need\" (Vasuvani 2017)](https://arxiv.org/pdf/1706.03762.pdf) a neural network architecture called _Transformer_ was introduced that was composed entirely of self attention layers, and had some other innovations regarding memory. \n",
    "\n",
    "![](images/transformer.png)\n",
    "\n",
    "In Feb 2019, a company called OpenAI introduced a variation of the transformer called GPT2 and [refused to release](https://slate.com/technology/2019/02/openai-gpt2-text-generating-algorithm-ai-dangerous.html) it _claiming it might destroy human society_ . This was a text generation model that could generate entire (_fake_) news articles from a one/few word prompt - think of it as autocomplete on steroids. They did eventually release it and is now available to try online: https://talktotransformer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
