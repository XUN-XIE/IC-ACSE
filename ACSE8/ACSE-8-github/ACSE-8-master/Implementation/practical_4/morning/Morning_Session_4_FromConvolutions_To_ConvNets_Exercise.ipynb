{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1061
    },
    "colab_type": "code",
    "id": "LL3vQPYkhzOy",
    "outputId": "3efa2667-03f7-40b9-cca4-fa824457943d"
   },
   "outputs": [],
   "source": [
    "!pip install pycm livelossplot\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "En9dBOn0hzPL"
   },
   "source": [
    "## Morning Session 4: From Convolutions to ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGURJ5EBhzPO"
   },
   "source": [
    "In today's exercise we will cover the following concepts:\n",
    "- Traditional Convolution based Computer Vision \n",
    "- Implement a traditional kernel-based methods visualized on a given image\n",
    "- Visualize various convolutional layers on a given image\n",
    "- Torch Convolutional layers \n",
    "- Implementation of LeNet\n",
    "- Simple ConvNet Excercise on MNIST - all training methods given, only change networks\n",
    "- Training with Data Augmentation\n",
    "- Transfer Learning for FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXJ6R5CBhzPQ"
   },
   "source": [
    "#### A few imports before we get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5-tUzUV2hzPU",
    "outputId": "fa10a5b0-f9c1-4b66-c16d-24ad90b6701e"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "from pycm import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tLRNHc8GOfR"
   },
   "source": [
    "### Mounting the google drive for later storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "Dg3oP6GHGOsX",
    "outputId": "68fe0cda-e5b4-4f3f-a8d7-930753367c0b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CEwibQeWjYQC"
   },
   "source": [
    "### Computer Vision - Convolutions as Feature Detectors\n",
    "\n",
    "In the following exercise we'll do some classical computer vision before moving to convolutional networks.\n",
    "A classical convolution operator is the so-called [Sobel-Filter](https://en.wikipedia.org/wiki/Sobel_operator)\n",
    "\n",
    "#### Task 1:\n",
    "Implement the Sobel Filter G_x from the provided website as a simple nn.conv2d operation.\n",
    "Hint: First instantiate a ```nn.Conv2d``` object with a single 3x3 kernel, padding=1, taking in a single image channel and outputting one channel.  \n",
    "Then modify the weight matrix to reflect the sobel filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "EwYSi4s8jYcj",
    "outputId": "871f76a9-662b-4655-9207-a26833c99748"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "url = \"https://cataas.com/cat\"\n",
    "response = requests.get(url)\n",
    "img = np.array(Image.open(BytesIO(response.content)).convert('L')).astype(float)\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3B-LEXgYjY1T"
   },
   "source": [
    "### Task 1: Implement the Sobel Filter $G_x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0CK1ONK9jZJG"
   },
   "source": [
    "### Some useful Convolutional Layers\n",
    "\n",
    "- Convolutions and Transposed Convolutions: Convolutional layers are parameterized by their kernel-weights and biases and are often used to reduce the spatial dimensionality.\n",
    "- Transposed Convolutions: Similar to convolutions, but these can increase the spatial dimensionality. [Blog on Problems with Tranposed Convolutions](https://distill.pub/2016/deconv-checkerboard/)\n",
    "- Upsampling Layers: Bilinear and Nearest Neighbor Upsampling\n",
    "- Pooling Layers: As discussed in the lecture notes - summarize spatial information - AveragePooling, MaxPooling, etc.\n",
    "- Dropout: Also exists in two (and more) dimensions: Can be use to regularise training of deep networks\n",
    "- Batchnormalization: Shift and center the distribution of the weights to a centered Gaussian distribution by keeping a running average of mini-batch properties.  \n",
    "Has been shown to help learning in very deep convolutional neural networks. [But it is not really well understood why this is the case.](https://arxiv.org/abs/1805.11604)\n",
    "\n",
    "The pytorch documentation is extremely well organised and I highly recommend you use it to your own advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "id": "vcKRCL3ojZT_",
    "outputId": "9338db18-260c-4914-8e0c-d2c2199a6ae3"
   },
   "outputs": [],
   "source": [
    "convolution = nn.Conv2d(1, 1, kernel_size=5, padding=2, stride=1)\n",
    "transposed_convolution = nn.ConvTranspose2d(1, 1, kernel_size=4, stride=2)\n",
    "upsampling = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "dropout = nn.Dropout2d(0.5)\n",
    "batchnorm = nn.BatchNorm2d(1)\n",
    "\n",
    "fig, axarr = plt.subplots(2, 3, figsize=(24, 12))\n",
    "for ax, op, name in zip(axarr.flatten(), [convolution, transposed_convolution, upsampling, pool, dropout, batchnorm], [\"conv\", \"conv_transposed\", \"upsample\", \"pool\", \"dropout\", \"batchnorm\"]):\n",
    "  filtered = op(x)\n",
    "  ax.imshow(filtered[0, 0].detach().numpy())\n",
    "  ax.set_title(name, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYn-cWg8jZcL"
   },
   "source": [
    "### Task 2: A simple Convolutional Network - LeNet-5\n",
    "![](https://www.researchgate.net/profile/Vladimir_Golovko3/publication/313808170/figure/fig3/AS:552880910618630@1508828489678/Architecture-of-LeNet-5.png)\n",
    "\n",
    "Let's now use our knowledge (and Pytorch documentation) Yann LeCun's LeNet-5 shown above.  \n",
    "You only have to implement the network itself, we provide you with the rest of the training code.  \n",
    "This is nearly identical to what you have already done in the previous session.  \n",
    "Assume that you will apply this to the MNIST dataset (28x28 grayscale images).  \n",
    "Here the network is shown to have input's of size 32x32, but we'll use zero-padding to achieve the same output size of the first convolutional layer.  \n",
    "All convolutional layers with trainable parameters should have a kernel-size=5.  \n",
    "In LeCun's [original paper](http://yann.lecun.com/exdb/lenet/) he did not use any padding, what is the size of the convolutional kernels?  \n",
    "Also, ReLU's did not exist at the time, but we'll use them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "X8Zfg2VHjZk-",
    "outputId": "4f2d0199-f7a9-4ba7-f7ff-7b37d7b85b77"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(LeNet5, self).__init__()\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return x\n",
    "  \n",
    "x = torch.randn((1, 1, 28, 28))\n",
    "model = LeNet5()\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_UioFmvFIDw"
   },
   "source": [
    "### The MNIST Dataset - Hello World of Deep-Learning - Now with ConvNets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "3K-yVuQvhzPs",
    "outputId": "c4c63663-6077-4e23-f55e-a1be38dda0aa"
   },
   "outputs": [],
   "source": [
    "mnist_train = MNIST(\"./\", download=True, train=True)\n",
    "mnist_test = MNIST(\"./\", download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIKWFh11AK_Y"
   },
   "source": [
    "### Instantiate and create a ```StratifiedShuffleSplit``` using sklearn.\n",
    "1. Create a ```sklearn.model_selection.StratifiedShuffleSplit``` object with 1-split and a test-size of 10%.\n",
    "2. Get the training and validation indices from the shuffel-split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "5zHKxYrY_15l",
    "outputId": "4a9bbdd1-cbfb-4b68-d879-39a701750caa"
   },
   "outputs": [],
   "source": [
    "shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(mnist_train.train_data, mnist_train.train_labels)\n",
    "indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euZacTUtALq7"
   },
   "source": [
    "### Splitting and normalize the data\n",
    "The original mnist data is given in gray-scale values between 0 and 255.\n",
    "You will need to write a normalisation method that takes in a ```torch.Tensor``` and performs normalisation.\n",
    "The mean of MNIST is 0.1307 and it's standard deviation is 0.3081 (after division by 255).\n",
    "\n",
    "Finally, torch likes all categorical data to be in a ```.long()``` format.\n",
    "Therefore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BY7_QOdPIDXm"
   },
   "outputs": [],
   "source": [
    "def apply_normalization(X):\n",
    "  X /= 255.\n",
    "  X -= 0.1307\n",
    "  X /= 0.3081\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "C33RJWTM_12w",
    "outputId": "56d94515-7184-4095-c3f1-fe0fc4bbd1d2"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = apply_normalization(mnist_train.train_data[indices[0]].float()), mnist_train.train_labels[indices[0]]\n",
    "X_val, y_val = apply_normalization(mnist_train.train_data[indices[1]].float()), mnist_train.train_labels[indices[1]]\n",
    "X_test, y_test =  apply_normalization(mnist_test.test_data.float()), mnist_test.test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RM9owG-yAMLv"
   },
   "source": [
    "### Instantiate a ```torch.utils.data.TensorDataset``` for training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IjL4N-P_1zV"
   },
   "outputs": [],
   "source": [
    "mnist_train = TensorDataset(X_train, y_train.long())\n",
    "mnist_validate = TensorDataset(X_val, y_val.long())\n",
    "mnist_test = TensorDataset(X_test, y_test.long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZ2qMRUGAEUA"
   },
   "source": [
    "Let's visualise an example of the images and check whether the data is normalised properly (compute .mean() and .std() on the training set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "JVlMiRAmoh0O",
    "outputId": "677fc0b3-3781-4961-e0a4-e3bb47e73e4f"
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0])\n",
    "print(X_train.mean(), X_train.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bcFPI8XKAOtl"
   },
   "source": [
    "### Provided Train, Validation and Evaluate Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRrehKxjhzQQ"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_loader):\n",
    "    model.train()\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    for X, y in data_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        a2 = model(X.view(-1, 28*28)) #What does this have to look like for our conv-net? Make the changes!\n",
    "        loss = criterion(a2, y)\n",
    "        loss.backward()\n",
    "        train_loss += loss*X.size(0)\n",
    "        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n",
    "        optimizer.step()  \n",
    "        \n",
    "    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n",
    "  \n",
    "def validate(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    validation_loss, validation_accuracy = 0., 0.\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 28*28)) #What does this have to look like for our conv-net? Make the changes!\n",
    "            loss = criterion(a2, y)\n",
    "            validation_loss += loss*X.size(0)\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n",
    "            \n",
    "    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n",
    "  \n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    ys, y_preds = [], []\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 28*28)) #What does this have to look like for our conv-net? Make the changes!\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            ys.append(y.cpu().numpy())\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_pehihNAQ9E"
   },
   "source": [
    " ### Set the hyperparameters of your model\n",
    "- Seed: 42\n",
    "- learning rate: 1e-2\n",
    "- Optimizer: SGD\n",
    "- momentum: 0.9\n",
    "- Number of Epochs: 30\n",
    "- Batchsize: 64\n",
    "- Test Batch Size (no effect on training apart from time): 1000\n",
    "- Shuffle the training set every epoch: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zvp3L6IGhzQj"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "lr = 1e-2\n",
    "momentum = 0.5\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QgnLf8fJ8nG"
   },
   "source": [
    "### Instantiate our model, optimizer and loss function\n",
    "Set the random number generator seed using ```set_seed``` to make everything reproducible.\n",
    "As a criterion use a sensible loss for the multi-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMEpCuC1EEFw"
   },
   "source": [
    "### Perform the training of the network and validation\n",
    "Here we provide you with a method to visualize both training and validation loss while training your networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "w9txG8N2hzQt",
    "outputId": "8d073e86-16ae-4597-e11a-e85a787036b8"
   },
   "outputs": [],
   "source": [
    "def train_model(momentum):\n",
    "  set_seed(seed)\n",
    "  model = LeNet5().to(device)\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "  validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "  test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "  \n",
    "  liveloss = PlotLosses()\n",
    "  for epoch in range(30):\n",
    "      logs = {}\n",
    "      train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "      logs['' + 'log loss'] = train_loss.item()\n",
    "      logs['' + 'accuracy'] = train_accuracy.item()\n",
    "\n",
    "      validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n",
    "      logs['val_' + 'log loss'] = validation_loss.item()\n",
    "      logs['val_' + 'accuracy'] = validation_accuracy.item()\n",
    "\n",
    "      liveloss.update(logs)\n",
    "      liveloss.draw()\n",
    "      \n",
    "  return model\n",
    "\n",
    "model = train_model(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kPt1sm8Mm_q"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "biyWFx8tAW9G"
   },
   "source": [
    "### Evaluate on the validation set and plot a confusion matrix\n",
    "This method performs the same as validate but doesn't report losses, but simply returns all predictions on a given dataset (training, validation, test-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1-MaI8F1hzQ7"
   },
   "outputs": [],
   "source": [
    "validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "y_pred, y_gt = evaluate(model, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "keGazNqkAXw9"
   },
   "source": [
    "### Plotting a confusion matrix\n",
    "\n",
    "We can use a confusion matrix to diagnose problems in our models.\n",
    "We may see for example that our model confuses 9's for 4's quite often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2501
    },
    "colab_type": "code",
    "id": "SWtJhZnDhzRE",
    "outputId": "1e62c465-0266-4137-b4be-2c3a03d3e340",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(actual_vector=y_gt, predict_vector=y_pred) # Create CM From Data\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-OG3gAcE2mr"
   },
   "source": [
    "### Given that you estimated your hyperparameters, train your model on the full dataset and evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "hsHixVU5hzRS",
    "outputId": "4954f25a-c433-4fce-f7da-aefa5f3103f3"
   },
   "outputs": [],
   "source": [
    "mnist_train = MNIST(\"./\", download=True, train=True)\n",
    "\n",
    "X_train, y_train = apply_normalization(mnist_train.train_data.float()), mnist_train.train_labels\n",
    "mnist_train = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "set_seed(seed)\n",
    "model = LeNet5().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "for epoch in range(n_epochs):\n",
    "    logs = {}\n",
    "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    logs['' + 'accuracy'] = train_accuracy.item()\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    "    logs['val_' + 'log loss'] = 0.\n",
    "    logs['val_' + 'accuracy'] = 0.\n",
    "\n",
    "test_loss, test_accuracy = validate(model, criterion, test_loader)    \n",
    "print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\n",
    "print(\"\")\n",
    "\n",
    "model_save_name = 'LeNet5_mnist_classifier.pt'\n",
    "path = F\"/content/gdrive/My Drive/models/{model_save_name}\" \n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhSLXauWHmZ2"
   },
   "source": [
    "### Custom Datasets and Transforms\n",
    "\n",
    "Pytorch allows us to simply extend the available Datasets to more custom functionality.\n",
    "Here we provide an example of such a custom dataset class.\n",
    "You can see that there are 3 functions we need to implement:\n",
    "- __init__(*args, **kwargs): this will handle everything prior to actually using the dataset\n",
    "- __len__(self): returns the length of the dataset i.e. the number of data items\n",
    "- __getitem__(self, idx): this method takes an index of a specific data item and returns that item.\n",
    "  - You can do whatever you want in these functions: apply transforms, normalize data, perform another computation etc.\n",
    "  - Here we also have the functionality to apply a set of [```torchvision.transforms```](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1klWll3s5i0g"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset \n",
    "\n",
    "class CustomImageTensorDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Tensor): A tensor containing the data e.g. images\n",
    "            targets (Tensor): A tensor containing all the labels\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, label = self.data[idx], self.targets[idx]\n",
    "        sample = sample.view(1, 28, 28).float()/255.\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dr0gAbQn5jE_"
   },
   "source": [
    "## Transforms - Data Augmentation\n",
    "\n",
    "Transforms can be used to perform manipulation of individual data prior to passing the data to our models.\n",
    "This is useful for:\n",
    " - Data-augmentation i.e. creating slightly modified instance of the data we have while preserving their labels.\n",
    " - Data Preprocessing: Such as Normalization, Histogram Equalization \n",
    " - Transforming Targets: You may have complex labels that should change together with changes in the preprocessing of the images\n",
    " \n",
    " Pytorch and especially torchvision provides a [number of transforms](https://pytorch.org/docs/stable/torchvision/index.html) for you to use!\n",
    " A nice tutorial on custom dataloaders and transforms can be found [here](https://github.com/utkuozbulak/pytorch-custom-dataset-examples).\n",
    " \n",
    " The (probably) most state-of-the-art library for image augmentation is [albumentations](https://github.com/albu/albumentations) which has been successfully applied in winning kaggle competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QVH3tekx8_q6"
   },
   "source": [
    "Using Albumentations is a bit more involved but you can find tutorials [here]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZN-QNTRE89u9"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomRotation, ToPILImage\n",
    "\n",
    "\n",
    "#Often we will want to apply more transformations at training time than test time, therefore here we have two different ones\n",
    "train_transform = Compose([\n",
    "    ToPILImage(),\n",
    "    RandomRotation(10),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.1307], std=[0.3081]), \n",
    "\n",
    "])\n",
    "\n",
    "#In Validation and Test Mode we only want to normalize our images, because they are already tensors\n",
    "validation_test_transform = Compose([\n",
    "    Normalize(mean=[0.1307], std=[0.3081])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyzpK5rw-Vdw"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVPMxLAC-s-2"
   },
   "source": [
    "## Task 3: Training with data augmentation\n",
    "\n",
    "- Instantiate a ```CustomImageTensorDataset``` with data from the MNIST dataset\n",
    "- Provide the training and validation and testing datasets with the right transforms\n",
    "- Train LeNet-5 with data-augmentation on a validation set, then train on the full training set and report accuracies. Did you improve the model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the ```CustomImageTensorDataset```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BmwMenXo-trt"
   },
   "source": [
    "### Training LeNet5 with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "BF4iL3uf-tzT",
    "outputId": "06e7829a-764a-4b47-f581-9a76ac3a0fe6"
   },
   "outputs": [],
   "source": [
    "def train_model_augmented(train_dataset, validation_dataset, momentum=0.5):\n",
    "  set_seed(seed)\n",
    "  model = LeNet5().to(device)\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "  validation_loader = DataLoader(validation_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "  liveloss = PlotLosses()\n",
    "  for epoch in range(30):\n",
    "      logs = {}\n",
    "      train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "      logs['' + 'log loss'] = train_loss.item()\n",
    "      logs['' + 'accuracy'] = train_accuracy.item()\n",
    "\n",
    "      validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n",
    "      logs['val_' + 'log loss'] = validation_loss.item()\n",
    "      logs['val_' + 'accuracy'] = validation_accuracy.item()\n",
    "\n",
    "      liveloss.update(logs)\n",
    "      liveloss.draw()\n",
    "      \n",
    "  return model\n",
    "\n",
    "model = train_model_augmented(custom_mnist_train, mnist_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AzB9QRC-EVzz"
   },
   "source": [
    "### Training on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "TZzTPoU7-Vov",
    "outputId": "2d1fe4a4-4b69-467a-c90c-7b8d0e5b1ec8"
   },
   "outputs": [],
   "source": [
    "mnist_train = MNIST(\"./\", download=True, train=True, transform=None)\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "set_seed(seed)\n",
    "model = LeNet5().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "for epoch in range(n_epochs):\n",
    "    logs = {}\n",
    "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    logs['' + 'accuracy'] = train_accuracy.item()\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    "    logs['val_' + 'log loss'] = 0.\n",
    "    logs['val_' + 'accuracy'] = 0.\n",
    "\n",
    "test_loss, test_accuracy = validate(model, criterion, test_loader)    \n",
    "print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\n",
    "print(\"\")\n",
    "\n",
    "model_save_name = 'LeNet5_mnist_classifier_with_augmentation.pt'\n",
    "path = F\"/content/gdrive/My Drive/models/{model_save_name}\" \n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRz54s_5GnMn"
   },
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssIYXFIjGn7A"
   },
   "source": [
    "We will now explore one of the arguably most useful practises when dealing with small-datasets and wanting to create a powerful classifier - transfer learning.  \n",
    "The basic principle behind is to leverage features learned on very large datasets and reuse them to perform tasks on smaller datasets.  \n",
    "To be able to apply transfer learning effectively, the data distribution of the data that a very powerful model was trained on should follow a similar distribution as the smaller dataset that we are trying to apply transfer learning to.    \n",
    "\n",
    "For example:  \n",
    "_We want to create a new classifier for cats and dogs given only a small set of say 100 training images of each category._\n",
    "\n",
    "Very deep neural networks that have been trained on ImageNet or CIFAR have similar categories in their dataset, say horses and maybe cows and many more categories of natural images.  \n",
    "The intuition is that since we've already learned a rich set of features on ImageNet, we can simply use a deep network as a feature extractor and only retrain the final layer of the networks to perform well at our task. So let's work our way towards transfer learning.\n",
    "\n",
    "\n",
    "## Task 4: Inspecting the features of pre-trained deep neural networks\n",
    "\n",
    "Pytorch provides users with a rich set of pre-trained neural network architectures. These have mostly been pre-trained on imagenet.   \n",
    "[```torchvision.models```](https://pytorch.org/docs/stable/torchvision/models.html) provides us with an interface to these pretrained deep neural networks.\n",
    "\n",
    "- Load a pretrained AlexNet model from ```torchvision.models```\n",
    "- Obtain the weight kernels of the first layer and display them (11x11 kernels shown as a matplotlib graph)\n",
    "- Remembering the earlier excercise on traditional computer vision kernels and edge detection, how could these come in handy when learning on new data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "gEgKXhsfa03b",
    "outputId": "f3f2881d-0741-43b3-f108-bb7dc6af5de4"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "alexnet = None\n",
    "print(alexnet.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "id": "oLN_0LtMbOd8",
    "outputId": "69fb7409-9d6b-49c1-d4c8-d96c0fe989e0"
   },
   "outputs": [],
   "source": [
    "for layer in alexnet.features.children():\n",
    "  weights = layer.weight.data\n",
    "  print(weights.size(), weights.min(), weights.max())\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y39SqwzLekvw"
   },
   "source": [
    "### Transfer learning from MNIST to FashionMNIST\n",
    "\n",
    "In the previous excercise we've investigated what the features of a very deep pre-trained network look like and learned about transfer learning.\n",
    "We've also already created a powerful classifier for MNIST, let's see if we can perform equally well on FashionMNIST with a limited dataset.\n",
    "Perform the following tasks:\n",
    "1. Load the FashionMNIST dataset\n",
    "2. Visualize a few examples of the dataset\n",
    "3. Create a Training Set and Validation Set by using ```StratifiedShuffleSplit``` and use only 10% of the training data for training.\n",
    "  - You will also need to normalize the FashionMNIST data to the same range as you did for the MNIST.\n",
    "4. Train a newly initialized LeNet5 on the 100% of the Training Data as a baseline\n",
    "5. Train a newly initialized LeNet5 on 10% of the training data\n",
    "6. Use the LeNet5 pretrained on MNIST (load the weights from the previous excercise) and perform the following:\n",
    "  - Replace the last layer of the LeNet with a new linear Layer\n",
    "  - Set the learning rates for all other layers to 0, except the final classifier\n",
    "  - Train on 10% of the data, validate against the remaining 90%, then predict on the full test set.\n",
    "  - How does your transfer learned model compare against learning from scratch?\n",
    "  - What happens when you train a full model on the 10% of data?\n",
    "  \n",
    "You can set different learning rates for different layers like so:\n",
    "```\n",
    "optim.SGD([\n",
    "{‘params’: model.base.parameters()},\n",
    "{‘params’: model.classifier.parameters(), ‘lr’: 1e-3}\n",
    "], lr=1e-2, momentum=0.9)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "HpqBDWTegydz",
    "outputId": "dbd939bf-4291-485d-fe10-b92b11e7ba13"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a newly initialized LeNet5 on FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "OjS87IIyiW4_",
    "outputId": "fee2bd1f-b37d-4c4d-8e33-c2a1518c9cfa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AeBUMRFj5H5"
   },
   "source": [
    "## 2. Train on a small subset of the full FashionMNIST (test_size=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "colab_type": "code",
    "id": "aHyiZZiCi7BE",
    "outputId": "a2e2ac61-0f1f-4d06-c2a2-2a50eae75f6a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRArCrs3j-12"
   },
   "source": [
    "## 3. Transfer Learning from a Network trained on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "qvoZPexxj93l",
    "outputId": "b707e4e8-6157-4aa9-eeed-46e798531a43"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ADxya2CMqua-"
   },
   "source": [
    "Almost as good as training from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DV5_Nma1HhNj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Morning_Session_4_FromConvolutions_To_ConvNets.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
