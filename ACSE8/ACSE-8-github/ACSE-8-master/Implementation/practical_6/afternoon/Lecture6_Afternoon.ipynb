{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "MujZxGrEIvhF",
    "outputId": "90cfac51-dbdf-48d7-9d24-763b263ae0fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: livelossplot in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (0.5.0)\n",
      "Requirement already satisfied: ipython in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from livelossplot) (7.12.0)\n",
      "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from livelossplot) (3.1.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from ipython->livelossplot) (46.0.0.post20200309)\n",
      "Requirement already satisfied: pygments in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from ipython->livelossplot) (2.5.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from ipython->livelossplot) (3.0.3)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from ipython->livelossplot) (0.14.1)\n",
      "Requirement already satisfied: backcall in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from ipython->livelossplot) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from ipython->livelossplot) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from ipython->livelossplot) (4.3.3)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from ipython->livelossplot) (0.1.0)\n",
      "Requirement already satisfied: decorator in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from ipython->livelossplot) (4.4.1)\n",
      "Requirement already satisfied: pickleshare in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from ipython->livelossplot) (0.7.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.18.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.8.1)\n",
      "Requirement already satisfied: wcwidth in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->livelossplot) (0.1.8)\n",
      "Requirement already satisfied: parso>=0.5.0 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython->livelossplot) (0.5.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.6.0)\n",
      "Requirement already satisfied: ipython-genutils in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\n",
      "Requirement already satisfied: six in /Users/xiexun/opt/anaconda3/lib/python3.7/site-packages (from traitlets>=4.2->ipython->livelossplot) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install livelossplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbiktUOBhU9h"
   },
   "source": [
    "# ACSE-8 Afternoon 6 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H61RBEfAj8kr"
   },
   "source": [
    "We do not have any exercises planned for Autoencoders/GANs. \n",
    "\n",
    "Feel free to use this afternoon to continue to work on your coursework submission due on Monday. \n",
    "\n",
    "If you have finished the coursework and are looking for another challenge, here's a code Olivier referenced in his RNN class. In the following code (adapted from [here](https://machinetalk.org/2019/02/08/text-generation-with-pytorch/)), we will train a model to generate text _in the style of_ the Harry Potter books. For this, we have provided you a text file _harry.txt_ which contains the full text of the first book - _Harry Potter and the Sorcerer's Stone_ (yes, unfortunately it is the American version).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGnxQhSBkxmM"
   },
   "source": [
    "First, we define our imports and some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKbrp3IthSxg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "flags = Namespace(\n",
    "    train_file='harry.txt',\n",
    "    seq_size=32,\n",
    "    batch_size=16,\n",
    "    embedding_size=64,\n",
    "    lstm_size=64,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['I', 'am'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path='checkpoint',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kj3_-Kc8k1V8"
   },
   "source": [
    "Next, we define some helper functions, first to load data from file,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AnkoH00BIxFl"
   },
   "outputs": [],
   "source": [
    "def get_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "    text = text.split()\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbvrQjMok7p4"
   },
   "source": [
    "...and the next one to divide the data into batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpkhgs6PI-PR"
   },
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWWBT6N9loqD"
   },
   "source": [
    "Now we define the RNN. [Look here](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) for an understanding of what an Embedding is and what it is doing for us here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdJgQtCCJAA9"
   },
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otSZWUGenQ0G"
   },
   "source": [
    "Some (last minute) hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tmi17ye9JG2T"
   },
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9cGTqWAMr3T-"
   },
   "source": [
    "Some housekeeping before we get started (the embedding size and lstm size are good targets for hyperparameter optimisation later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "RS8hKzkpJNCl",
    "outputId": "23d30cd8-c328-48e6-8b50-c8aaf67985fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 11895\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
    "    flags.train_file, flags.batch_size, flags.seq_size)\n",
    "\n",
    "net = RNNModule(n_vocab, flags.embedding_size, flags.lstm_size)\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWE63wZEul1X"
   },
   "source": [
    "Finally we are ready to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "tTstvnw5Khqj",
    "outputId": "facd4687-409c-4566-b23c-de2df6bd8a58"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\tloss             \t (min:    1.620, max:    8.836, cur:    1.620)\n"
     ]
    }
   ],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "logs = {}\n",
    "for e in range(50):\n",
    "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "        state_h, state_c = net.zero_state(flags.batch_size)\n",
    "        \n",
    "        # Transfer data to GPU\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        for x, y in batches:\n",
    "            iteration += 1\n",
    "            \n",
    "            # Tell it we are in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Reset all gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transfer data to GPU\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "\n",
    "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # Perform back-propagation\n",
    "            loss.backward()\n",
    "\n",
    "            _ = torch.nn.utils.clip_grad_norm_(\n",
    "                net.parameters(), flags.gradients_norm)\n",
    "\n",
    "            # Update the network's parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        logs['loss'] = loss_value\n",
    "        liveloss.update(logs)\n",
    "        liveloss.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rn2Rs3MxwZ3e"
   },
   "source": [
    "With the model trained, we can finally get to the best part: Generating text! This function samples from the output distribution of our trained model to generate text. Try it out yourself in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHpCS-buN02V"
   },
   "outputs": [],
   "source": [
    "from textwrap import fill\n",
    "\n",
    "\n",
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0]) # Could be improved\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "\n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0]) # Could be improved\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    print(fill(' '.join(words), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_KIEmBpxEEE"
   },
   "source": [
    "Here comes the fun. Enter any two \"seed words\" in the next cell to get the model to generate some text starting with those words. \n",
    "\n",
    "You might notice that the initial model does not produce very good text. Here's some things that could be improved:\n",
    " - The sampling bit of the predict function in the above cell doesn't seem to be doing a great job at sampling from the posterior distribution. Can you improve it? (Remember: You don't need to retrain to see improvements from this)\n",
    " - Using the final value of the loss to guide you, can you find better values for the embedding size/lstm size hyperparameters above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "HwmOmTMFdVzn",
    "outputId": "89e3acf5-dcae-4b39-e83f-b35f9faea8d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lord Voldemort got out, clapped loudest. by one, and then came pelting out onto their way through\n",
      "the barrier. It yelled in her mouth, perhaps Hogwarts. The old man pulled the other day were\n",
      "learning old enough, passed Professor McGonagall was still safe. toward himself on it and he was\n",
      "looking forward through his eyes. \"A note wasn't much space from being corridors with one of their\n",
      "men was guarding three of them started to drift Hagrid's jacket and an empty except Harry had gone\n",
      "only joking, I never told them what this wasn't inside all as your family's one of the red-haired\n"
     ]
    }
   ],
   "source": [
    "predict(device, net, [\"Lord\", \"Voldemort\"], n_vocab, vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18kthR-8dbkm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lecture6-Afternoon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
