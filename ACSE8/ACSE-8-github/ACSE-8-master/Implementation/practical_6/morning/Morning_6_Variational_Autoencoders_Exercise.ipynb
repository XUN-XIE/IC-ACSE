{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "colab_type": "code",
    "id": "LL3vQPYkhzOy",
    "outputId": "403d5f35-54a5-4d89-8588-0582a2262693"
   },
   "outputs": [],
   "source": [
    "!pip install livelossplot\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "En9dBOn0hzPL"
   },
   "source": [
    "## Morning Session 6: Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGURJ5EBhzPO"
   },
   "source": [
    "In this session you will implement a simple variational autoencoder based on the original paper by Kingma and Welling.  \n",
    "\n",
    "The network will be based on a fully connected deep neural network.  \n",
    "The dataset will be the MNIST dataset.  \n",
    "We will use stochastic gradient descent to maximize the ELBO i.e. minimizing the KL-Divergence.  \n",
    "\n",
    "You will have to refer to the notes of the previous day for technical reference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXJ6R5CBhzPQ"
   },
   "source": [
    "#### A few imports before we get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5-tUzUV2hzPU",
    "outputId": "bcf8ba7a-724d-4faf-bb99-8e329360bd7d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import random \n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tLRNHc8GOfR"
   },
   "source": [
    "### Mounting the google drive for later storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "Dg3oP6GHGOsX",
    "outputId": "6a634d78-c6ef-4fd9-e178-f4e26de396ef"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CEwibQeWjYQC"
   },
   "source": [
    "### Exercise 1: Implementing the Deep Neural Networks\n",
    "\n",
    "Variational Autoencoders and Autoencoders more generally are a special type of neural network that take as an input a training example, perform a (non-)linear transformation of that data and outputs a reconstructed version of the input.\n",
    "\n",
    "The first step of the exercise will be to implement a simple deep neural network for a variational autoencoder.\n",
    "\n",
    "The general structure will be: _Encoder_ -> _Latent Space_ -> _Decoder_\n",
    "\n",
    "![](https://lilianweng.github.io/lil-log/assets/images/vae-gaussian.png)\n",
    "_Credit to Lilian Weng and her [blog](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html) on VAE's_\n",
    "\n",
    "The encoder should consist of 2 hidden layers and a layer for the mean and the log-variance of the latent space variables i.e. you will need at least 4 ```nn.Linear``` layers to implement your encoder. We will use the following number of neurons for each layer:\n",
    "1. Layer 1: 784->512 neurons\n",
    "2. Layer 2: 512->256 neurons\n",
    "3. a) Layer 3 (Mean): 256->Size of Latent Space\n",
    "3. b) Layer 3 (Log Var): 256-> Size of Latent Space\n",
    "The decoder should consist of 3 hidden layers with following architecture:\n",
    "\n",
    "1. Layer 4: Size of Latent Space -> 256 neurons\n",
    "2. Layer 5: 256 -> 512 neurons\n",
    "3. Layer6: 512 -> 784 neurons\n",
    "\n",
    "Use ReLU activation functions for all the layers except the layers representing the mean and log-variance as well as the final output layer of the reconstruction.\n",
    "No activation is required on the mean and log-var layers - a sigmoid should be used on the final output layer.\n",
    "\n",
    "Use the available template ```nn.Module``` to implement your networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtC12XmetYck"
   },
   "source": [
    "#### Exercise Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "UYJMYuNItYp6",
    "outputId": "f6493181-4276-4d29-a385-74c248d51767"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder torch.nn layers\n",
    "        \n",
    "        # decoder torch.nn layers\n",
    "\n",
    "        # activations        \n",
    "        self.activation = False\n",
    "        self.sigmoid = False\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        # Implement the Encoder\n",
    "        return mu, logvar\n",
    "    \n",
    "    def latent_space(self, mu, log_var):\n",
    "      # The reparametrization trick in action\n",
    "        if self.training:\n",
    "          std = torch.exp(0.5*log_var)\n",
    "          eps = torch.randn_like(std)\n",
    "          z = eps.mul(std).add_(mu) \n",
    "        else:\n",
    "          z = mu\n",
    "        return z\n",
    "        \n",
    "    def decoder(self, z):\n",
    "      # Implement the decoder\n",
    "        return reconstruction\n",
    "    \n",
    "    def forward(self, x):\n",
    "      #Encode the given image\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        \n",
    "        # Create the latent-variable representation\n",
    "        z = self.latent_space(mu, log_var)\n",
    "        \n",
    "        # Reconstruct the image from the latent-variables\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction.view(-1, 1, 28, 28), z, mu, log_var\n",
    "\n",
    "# build model and test it\n",
    "x = torch.randn((1, 1, 28, 28))\n",
    "model = VAE()\n",
    "x_, z, mu, logvar = model(x)\n",
    "print(x_.size(), z.size(), mu.size(), logvar.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_UioFmvFIDw"
   },
   "source": [
    "### Exercise 2: Implement a custom Dataloader for the VAE training\n",
    "\n",
    "The custom dataset should only take images of the MNIST dataset and output MNIST images, no need to output labels or use transforms.\n",
    "Remember: The dataset is represented as grayscale values between 0-255. What we want is a representation between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qxt1gfyjuery"
   },
   "source": [
    "#### Exercise Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "o8pIfcpaud22",
    "outputId": "f82ff60b-e1df-452d-d702-67451fc5b674"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset \n",
    "\n",
    "mnist_train_dset = MNIST(\"./\", download=True, train=True)\n",
    "mnist_test_dset = MNIST(\"./\", download=True, train=False)\n",
    "\n",
    "class CustomVAETensorDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Tensor): A tensor containing the data e.g. images\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "      \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return sample\n",
    "\n",
    "mnist_train = CustomVAETensorDataset(mnist_train_dset.train_data)\n",
    "mnist_test = CustomVAETensorDataset(mnist_test_dset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNwYiyzTvGaF"
   },
   "source": [
    "### Exercise 3: Implement the VAE loss functions\n",
    "\n",
    "$$ KLD = -\\frac{1}{2}\\sum_i^{Latent~Dim}\\{ 1+\\log \\sigma^2_i(x) - \\mu_i(x)^2 - e^{\\log\\sigma^2_i(x)} \\}$$\n",
    "$$ Reconstruction \\ Loss = BinaryCrossEntropy(x, x')$$\n",
    "\n",
    "Don't compute the mean in the BinaryCrossEntropy (size_averge=False) because we are treating each pixel as a random variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59z4Yo78vGlc"
   },
   "outputs": [],
   "source": [
    "def kl_divergence(mu, logvar):\n",
    "    KLD = 0\n",
    "    return KLD\n",
    "  \n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    reconstruction_loss, kld = 0, 0\n",
    "    return reconstruction_loss, kld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bcFPI8XKAOtl"
   },
   "source": [
    "### Provided Train, Validation and Evaluate Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRrehKxjhzQQ"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_loader):\n",
    "    model.train()\n",
    "    train_bce, train_kld = 0, 0\n",
    "    for X in data_loader:\n",
    "        X = X.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_, z, mu, logvar = model(X.float())\n",
    "        bce, kld = loss_function(x_, X.float(), mu, logvar)\n",
    "        \n",
    "        total_loss = bce+kld\n",
    "        total_loss.backward()\n",
    "        \n",
    "        train_bce += bce\n",
    "        train_kld += kld\n",
    "        \n",
    "        optimizer.step()  \n",
    "        \n",
    "    return train_bce/len(data_loader.dataset), train_kld/len(data_loader.dataset)\n",
    "  \n",
    "def validate(model, data_loader):\n",
    "    model.train()\n",
    "    validation_bce, validation_kld = 0, 0\n",
    "    for X in data_loader:\n",
    "        with torch.no_grad():\n",
    "          X = X.to(device)\n",
    "          x_, z, mu, logvar = model(X)\n",
    "          bce, kld = loss_function(x_, X, mu, logvar)\n",
    "\n",
    "          total_loss = bce+kld\n",
    "\n",
    "          validation_bce += bce\n",
    "          validation_kld += kld\n",
    "        \n",
    "    return validation_bce/len(data_loader.dataset), validation_kld/len(data_loader.dataset)\n",
    "  \n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    xs, zs = [], []\n",
    "    for X in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X = X.to(device)\n",
    "            x_, z, _, _ = model(X)\n",
    "\n",
    "            xs.append(x_.cpu().numpy())\n",
    "            zs.append(z.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(xs, 0),  np.concatenate(zs, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_pehihNAQ9E"
   },
   "source": [
    " ### Set the hyperparameters of your model\n",
    "- Seed: 42\n",
    "- learning rate: 1e-3\n",
    "- Optimizer: Adam\n",
    "- Number of Epochs: 300\n",
    "- Batchsize: 128\n",
    "- Test Batch Size (no effect on training apart from time): 1024\n",
    "- Shuffle the training set every epoch: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zvp3L6IGhzQj"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "test_batch_size = 1024\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QgnLf8fJ8nG"
   },
   "source": [
    "### Instantiate our model, optimizer and loss function\n",
    "Set the random number generator seed using ```set_seed``` to make everything reproducible.\n",
    "As a criterion use a sensible loss for the multi-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMEpCuC1EEFw"
   },
   "source": [
    "### Excercise 4: Perform the training of the variational autoencoder\n",
    "Train your VAE and visualize the reconstruction loss and the KL-Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "w9txG8N2hzQt",
    "outputId": "1db8499f-d85b-46dc-c035-ab1340642705"
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    set_seed(42)\n",
    "    model = VAE().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    min_loss = 1e6-1\n",
    "    liveloss = PlotLosses()\n",
    "    for epoch in range(n_epochs):\n",
    "        logs = {}\n",
    "        train_loss, train_kld = train(model, optimizer, train_loader)\n",
    "\n",
    "        logs['' + 'bce'] = train_loss.item()\n",
    "        logs['' + 'kld'] = train_kld.item()\n",
    "\n",
    "        validation_loss, validation_kld = validate(model, test_loader)\n",
    "        logs['val_' + 'bce'] = validation_loss.item()\n",
    "        logs['val_' + 'kld'] = validation_kld.item()\n",
    "\n",
    "        liveloss.update(logs)\n",
    "        liveloss.draw()\n",
    "\n",
    "        #Store best validation loss model\n",
    "        if validation_loss < min_loss:\n",
    "            torch.save(model.state_dict(), F\"/content/gdrive/My Drive/models/mnist_vae_\"+str(epoch)+\".pth\")\n",
    "            min_loss = validation_loss\n",
    "      \n",
    "    return model\n",
    "\n",
    "model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5ltNm3ivkGM"
   },
   "source": [
    "### Exercise 5: Load the best model checkpoint and inspection of results\n",
    "\n",
    "Perform the following tasks:\n",
    "\n",
    "1. Load the best model checkpoint based on the validation loss\n",
    "2. Apply the VAE to the test dataset and store the reconstructions and latent-variables.\n",
    "3. Plot the first 100 test set images and their reconstructions\n",
    "4. Plot the 10000 images of the test set in their latent-space representation and use color to highlight the class labels of the MNIST digits.\n",
    "  - What do you observe?\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Morning_Session_6_VAEs.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
